{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70b5df",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from AddData import add_player_understat_data, add_team_understat_data, add_opp_understat_data\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "from GetClubELOData import add_elo\n",
    "from GetFPLData import load_fpl_data\n",
    "import GetUnderstatData as getus\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from priority_list import priority_list\n",
    "import pytz\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tools as st\n",
    "import statsmodels.graphics.api as smg\n",
    "from tqdm.notebook import tqdm\n",
    "from Utils import gof_plots, split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2d878",
   "metadata": {},
   "source": [
    "# Getting the data\n",
    "\n",
    "The first thing to do here is to get the data that we will be using in the model. The data will come from three sources:\n",
    "1. Fantasy Premier League API. This has been acquired via the excellent [Fantasy Premier League Github repo](https://github.com/vaastav/Fantasy-Premier-League) courtesy of Vaastav and is used to provide individual player statistics collected for the FPL game by the Premier League.\n",
    "2. Understat. This is primarily used as a source of expected data for both players and teams.\n",
    "3. ClubELO.com. This is used to add ELO metrics for the player's team and the opposition.\n",
    "\n",
    "Having got the data, we set up a model dataframe by using lagged statistics from the previous 4 matches for a player, including their team statistics, as well as statistics about the upcoming opposition and their previous games. The main drawback to this approach is that if we want to use it to forecast points for players who have not played at least 4 games this season, data from the previous season would have to be used. This might not take into account structural changes that have occurred over the break such as transfers that impact a team's strength, or the player themselves changing teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980cf6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpl_df = pd.read_csv(\"./Data/model_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b940f",
   "metadata": {},
   "source": [
    "Since each position acquires points in a different manner within FPL, I expect the models produced for each position to differ significantly. In order to simplify the process I will split the modelling and analysis of the data according to position (GK, DEF, MID and FWD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e9908",
   "metadata": {},
   "source": [
    "# Exploring the data\n",
    "\n",
    "It is of interest to compare the difference in FPL points gained by players starting a match in contrast to substitutes or players that fail to make the matchday squad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b52c1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create dict of points distributions by position for starters and non-starters\n",
    "element_types_list = fpl_df.element_type.unique().tolist()\n",
    "element_types_list.sort()\n",
    "starters_idxs = {}\n",
    "non_starters_idxs = {}\n",
    "total_points_dist_dict = {element_type : {} for element_type in element_types_list}\n",
    "for element_type in element_types_list:\n",
    "    # Find index of starters and non-starters for players in given position\n",
    "    starters_idxs[element_type] = fpl_df.loc[((fpl_df['element_type'] == element_type) & (fpl_df['starts'] == 1))].index\n",
    "    non_starters_idxs[element_type] = fpl_df.loc[((fpl_df['element_type'] == element_type) & (fpl_df['starts'] == 0))].index\n",
    "    \n",
    "    starters = starters_idxs[element_type]\n",
    "    non_starters = non_starters_idxs[element_type]\n",
    "    \n",
    "    total_points_dist_dict[element_type]['starters'] = fpl_df.loc[starters, 'total_points'].value_counts(normalize=True)\n",
    "    total_points_dist_dict[element_type]['non-starters'] = fpl_df.loc[non_starters, 'total_points'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702d707",
   "metadata": {
    "scrolled": false,
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Create bar plots for returns of starters and non-starters in each position\n",
    "element_type_dict = {1 : 'GK', 2 : 'DEF', 3 : 'MID', 4 : 'FWD'}\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8), sharey='all', layout='constrained')\n",
    "axs = axs.flatten()\n",
    "\n",
    "for ax, element_type in zip(axs, element_types_list):\n",
    "    total_points = total_points_dist_dict[element_type]\n",
    "    ax.bar(total_points['starters'].index, total_points['starters'].values, alpha=0.5, label=\"Starters\")\n",
    "    ax.bar(total_points['non-starters'].index, total_points['non-starters'].values, alpha=0.5, label=\"Non-starters\", color='r')\n",
    "    ax.set_title(element_type_dict[element_type], fontsize=14)\n",
    "\n",
    "axs[0].set_ylabel(\"Probability\")\n",
    "axs[2].set_xlabel(\"Total Points\")\n",
    "axs[2].set_ylabel(\"Probability\")\n",
    "axs[3].set_xlabel(\"Total Points\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=(0.85, 0.875), fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e7a65",
   "metadata": {},
   "source": [
    "For each position the distribution of points for starters and non-starters differs significantly, with players not starting the match scoring fewer points on average. The mean points for players starting and not\n",
    "\n",
    "In particular, players who do not start the match are more likely to score zero points than those starting the game. Since most players not starting a match are simply scoring zero points, my aim will be to predict the points scored by a player given that they start a match. For this model to aid with player selection in the Fantasy Premier League game, it will then be necessary to estimate the probability that a player does indeed start the upcoming match. This will likely be out of the scope of this project.\n",
    "\n",
    "| Position | Avg. score of starters | Avg. score of non-starters |\n",
    "| --- | --- | --- |\n",
    "| GK | 3.608 | 0.005 |\n",
    "| DEF | 3.083 | 0.098 |\n",
    "| MID | 3.442 | 0.266 |\n",
    "| FWD | 3.927 | 0.344 |\n",
    "| **Aggregate** | **3.374** | **0.191** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d731d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_na = fpl_df.isna().any(axis=1).astype(int).value_counts(normalize=True)[1]\n",
    "print(\"{:.1f}% of rows contain null values\".format(has_na*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b83021",
   "metadata": {},
   "source": [
    "There are a number of methods for dealing with the null values in the data. Since these have been systematically induced by the lagging process when creating the dataset, I have chosen to drop the rows containing the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee9456",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Drop null values and players who did not start from the model dataframe\n",
    "model_df = fpl_df.copy()\n",
    "model_df.dropna(inplace=True)\n",
    "model_df.drop(model_df.loc[model_df.starts == 0].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317370d7",
   "metadata": {},
   "source": [
    "There are a couple of ways that I can think to possibly improve the features of the data. The first is to consider the number of incoming/outgoing transfers for a player as a percentage of the total number of incoming/outgoing tranfers for the player's position. To my mind, this should reduce the effect of the varying number of active players across the season on the number of total transfers for a player and give a better indication of how in-demand the player truly is. Another idea is to introduce a new feature which ranks players based on the number of net incoming transfers in the build-up to the gameweek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad729ada",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Add new features that require grouping to the model dataframe \n",
    "def create_feature_cols(df):\n",
    "    df1 = df.drop_duplicates([\"element\"])\n",
    "    df1['transfers_in_pct'] = 100 * df1.transfers_in/df1.transfers_in.sum()\n",
    "    df1['transfers_out_pct'] = 100 * df1.transfers_out/df1.transfers_out.sum()\n",
    "    df1['transfers_balance_rank'] = df1.transfers_balance.rank()\n",
    "    \n",
    "    df = df.merge(df1[['element', 'transfers_in_pct', 'transfers_out_pct', 'transfers_balance_rank']],\n",
    "                  how='left',\n",
    "                  on='element')\n",
    "    return df[['element', 'transfers_in_pct', 'transfers_out_pct', 'transfers_balance_rank']]\n",
    "\n",
    "new_features_df = model_df.groupby([\"season_x\", \"round\", \"element_type\"]).apply(create_feature_cols)\n",
    "\n",
    "new_features_df.index = new_features_df.index.droplevel(3)\n",
    "new_features_df.reset_index(inplace=True)\n",
    "\n",
    "model_df = pd.merge(model_df, new_features_df)\n",
    "model_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Add other features to the dataframe\n",
    "model_df['delta_value'] = model_df['value'] - model_df['value_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb20a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by time\n",
    "model_df.sort_values(\"kickoff_date\", ascending=True)\n",
    "\n",
    "# Drop columns not to be included in the models\n",
    "drop_cols = ['assists', 'bonus', 'bps', 'clean_sheets', 'creativity', 'element', 'fixture', 'goals_conceded', 'goals_scored', \n",
    "             'ict_index', 'influence', 'kickoff_time', 'minutes', 'opponent_team', 'own_goals', 'penalties_missed', \n",
    "             'penalties_saved', 'red_cards', 'round', 'saves', 'team_a_score', 'team_h_score', 'threat', 'transfers_balance', \n",
    "             'yellow_cards', 'name', 'season_x', 'opp_team_name', 'kickoff_datetime', 'kickoff_date', 'team_x', 'starts', \n",
    "             'shots', 'xG', 'position', 'xA', 'key_passes', 'npg', 'npxG', 'xGChain', 'xGBuildup', 'team_xG', 'team_xGA', \n",
    "             'team_npxG', 'team_npxGA', 'team_deep', 'team_deep_allowed', 'team_scored', 'team_missed', 'team_xpts', \n",
    "             'team_result', 'team_wins', 'team_draws', 'team_loses', 'team_pts', 'team_npxGD', 'team_ppda_att', \n",
    "             'team_ppda_def', 'team_ppda_allowed_att', 'team_ppda_allowed_def']\n",
    "\n",
    "model_df.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "# Set all remaining columns to be numeric\n",
    "model_df = model_df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data by position for modelling\n",
    "gk_df = model_df.loc[model_df.element_type == 1].copy().drop('element_type', axis=1)\n",
    "def_df = model_df.loc[model_df.element_type == 2].copy().drop('element_type', axis=1)\n",
    "mid_df = model_df.loc[model_df.element_type == 3].copy().drop('element_type', axis=1)\n",
    "fwd_df = model_df.loc[model_df.element_type == 4].copy().drop('element_type', axis=1)\n",
    "\n",
    "# Split each dataframe and response vector into a training set and a test set\n",
    "X_train_gk, X_test_gk, y_train_gk, y_test_gk = train_test_split(\n",
    "    gk_df.drop('total_points', axis=1), \n",
    "    gk_df['total_points'], \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_train_def, X_test_def, y_train_def, y_test_def = train_test_split(\n",
    "    def_df.drop('total_points', axis=1), \n",
    "    def_df['total_points'], \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_train_mid, X_test_mid, y_train_mid, y_test_mid = train_test_split(\n",
    "    mid_df.drop('total_points', axis=1), \n",
    "    mid_df['total_points'], \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_train_fwd, X_test_fwd, y_train_fwd, y_test_fwd = train_test_split(\n",
    "    fwd_df.drop('total_points', axis=1), \n",
    "    fwd_df['total_points'], \n",
    "    test_size=0.2, \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9819c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return normalized design matrices\n",
    "def normalize(train_df, test_df):    \n",
    "    X_mean = train_df.mean()\n",
    "    X_std = train_df.std()\n",
    "    X_train = ((train_df - X_mean)/X_std)\n",
    "    X_test = ((test_df - X_mean)/X_std)\n",
    "    \n",
    "    # Find columns that have null values in X_train, which correspond to constant columns in train_df, and drop from X_train\n",
    "    # and X_test\n",
    "    na_cols = X_train.columns[X_train.isna().any(axis=0)]\n",
    "    X_train.drop(na_cols, axis=1, inplace=True)\n",
    "    X_test.drop(na_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Add constant term\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    X_test = sm.add_constant(X_test, has_constant='add')\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gk1, X_test_gk1 = normalize(X_train_gk, X_test_gk)\n",
    "X_train_def1, X_test_def1 = normalize(X_train_def, X_test_def)\n",
    "X_train_mid1, X_test_mid1 = normalize(X_train_mid, X_test_mid)\n",
    "X_train_fwd1, X_test_fwd1 = normalize(X_train_fwd, X_test_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573a6bc",
   "metadata": {},
   "source": [
    "# Goalkeeper modelling\n",
    "\n",
    "I begin by examining the correlation between columns in the dataframe. Having highly correlated columns in the dataframe can create numerical instability in the regression estimates. In this context, since several statistics exhibit autocorrelation across various lags, this manifests itself as correlation between columns in the design matrix. In order to establish a parsimonious model, I will remove columns beginning with those further back in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = X_train_gk1.corr()\n",
    "\n",
    "corr_threshold = 0.9\n",
    "corr_pairs = []\n",
    "is_corr = (corr_mat >= corr_threshold)\n",
    "for i in range(is_corr.shape[0]):\n",
    "    for j in range(i+1, is_corr.shape[1]):\n",
    "        if is_corr.iloc[i, j]:\n",
    "            corr_pairs.append([\"{}\".format(is_corr.index[i]), \"{}\".format(is_corr.columns[j])])\n",
    "            \n",
    "print(\"There are {} column pairs with correlation greater than {}\".format(len(corr_pairs), corr_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1db5c5",
   "metadata": {},
   "source": [
    "I will avoid showing all pairs of columns exceeding the threshold but the following table gives a snapshot of some of the correlation data:\n",
    "\n",
    "| Column pair | Correlation |\n",
    "| --- | --- |\n",
    "| selected, selected_1 | 0.994 |\n",
    "| selected, selected_2 | 0.985 |\n",
    "| selected, selected_3 | 0.975 |\n",
    "| selected, selected_4 | 0.963 |\n",
    "| value, value_1 | 0.999 |\n",
    "| value, value_2 | 0.998 |\n",
    "| value, value_3 | 0.996 |\n",
    "| value, value_4 | 0.995 |\n",
    "| creativity_1, key_passes_1 | 0.965 |\n",
    "| goals_conceded_1, team_missed_1 | 0.907 |\n",
    "| ict_index_1, influence_1 | 0.993 |\n",
    "| ict_index_1, saves_1 | 0.938 |\n",
    "| influence_1, saves_1 | 0.948 |\n",
    "| minutes_1, starts_1 | 0.989 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated columns\n",
    "drop_cols = ([\"opp_npxGA_{}\".format(x) for x in range(1, 5)] +\n",
    "             [\"opp_npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"team_npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"team_npxGA_{}\".format(x) for x in range(1, 5)] +\n",
    "             [\"selected_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"value_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"influence_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"ict_index_{}\".format(x) for x in range(1, 5)] +\n",
    "             [\"creativity_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"threat_{}\".format(x) for x in range(1, 5)] +\n",
    "             [\"xGBuildup_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"bps_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"shots_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"starts_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"npxG_{}\".format(x) for x in range(1, 5)]\n",
    "            )\n",
    "\n",
    "X_train_gk2 = X_train_gk1.drop(drop_cols, axis=1)\n",
    "X_test_gk2 = X_test_gk1.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100c3d4",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05bd8f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit an initial linear model to the training data\n",
    "ols_model_gk = sm.OLS(y_train_gk, X_train_gk2)\n",
    "ols_res_gk = ols_model_gk.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb03db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_gk_resid = ols_res_gk.resid_pearson\n",
    "ols_gk_fv = ols_res_gk.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9cb2d6",
   "metadata": {},
   "source": [
    "From the QQ plot below we see that the standardised residuals do not appear to follow a standard Normal distribution, indicating that the model is misspecified. The plot of standardised residuals appears to indicate heteroskedasticity in the standardised residuals, again violating the assumptions of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5db43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gof_plots(y_train_gk, ols_gk_resid, ols_gk_fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a156d",
   "metadata": {},
   "source": [
    "I now want to apply the model to the test data in order to assess its performance. As a baseline model I will use the average value from the training data and assign this as the predicted value for all test set data. Then, we can compare the performance of future models against this uninformed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test MAE and MSE under the use of the training mean as a constant model\n",
    "gk_baseline_mae = np.mean(abs(y_train_gk.mean() - y_test_gk))\n",
    "gk_baseline_mse = np.mean((y_train_gk.mean() - y_test_gk)**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(gk_baseline_mae, gk_baseline_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4295e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted values for the test data\n",
    "X_test_gk2 = X_test_gk1[X_train_gk2.columns]\n",
    "ols_gk_test_predict = ols_res_gk.predict(X_test_gk2)\n",
    "ols_gk_test_resid = ols_gk_test_predict - y_test_gk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61043ff1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_gk, ols_gk_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ols_gk_test_predict, ols_gk_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0df332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test MAE and MSE\n",
    "ols_gk_test_mae = np.mean(abs(ols_gk_test_resid))\n",
    "ols_gk_test_mse = np.mean(ols_gk_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(ols_gk_test_mae, ols_gk_test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a9e50",
   "metadata": {},
   "source": [
    "So the initial linear model performs worse on the test set than an uninformed model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa752c4",
   "metadata": {},
   "source": [
    "### Transforming variables\n",
    "\n",
    "From the diagnostic plots above we can see that the model is misspecified. The QQ plot appears to indicate that the response data is skewed. In order to attempt to resolve this I will apply a Box-Cox transformation to the data. Since the Box-Cox transformation only applies to positive data I will remove the negative observations and, shift the response and apply the Box-Cox transformation to the resulting data. I believe that it is reasonable as negative outcomes constitute only 0.81% of responses across the training and the test data, so the removal of these observations from the training set should have little effect on the overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9546b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 1\n",
    "y_train_gk_boxcox = y_train_gk.drop(y_train_gk.loc[y_train_gk < 0].index) + shift\n",
    "y_train_gk_boxcox_index = y_train_gk_boxcox.index\n",
    "X_train_gk_boxcox = X_train_gk2.drop(y_train_gk.loc[y_train_gk < 0].index)\n",
    "\n",
    "y_train_gk_boxcox, lambda_gk = stats.boxcox(y_train_gk_boxcox)\n",
    "\n",
    "y_train_gk_boxcox = pd.Series(y_train_gk_boxcox, y_train_gk_boxcox_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da409653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stepwise backward elimination, removing variables whose coefficient estimates do not have p-values falling below\n",
    "# the specified threshold\n",
    "priority_list_gk = priority_list.copy()\n",
    "dropped_cols = []\n",
    "X_train_gk_temp = X_train_gk_boxcox.copy()\n",
    "selection_complete = False\n",
    "p_value_threshold = 0.1\n",
    "\n",
    "while not selection_complete:\n",
    "    model_temp = sm.OLS(y_train_gk_boxcox, X_train_gk_temp)\n",
    "    res_temp = model_temp.fit()\n",
    "    drop_candidates = res_temp.pvalues.loc[res_temp.pvalues > p_value_threshold]\n",
    "    if len(drop_candidates) == 0:\n",
    "        selection_complete = True\n",
    "        continue\n",
    "    \n",
    "    drop_col = priority_list_gk.pop([col in drop_candidates for col in priority_list_gk].index(True))\n",
    "    dropped_cols.append(drop_col)\n",
    "    X_train_gk_temp.drop(drop_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ca51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine model chosen by backward elimination\n",
    "X_train_gk_bs = sm.add_constant(X_train_gk_temp)\n",
    "ols_bs_model_gk = sm.OLS(y_train_gk_boxcox, X_train_gk_bs)\n",
    "ols_bs_res_gk = ols_bs_model_gk.fit()\n",
    "\n",
    "print(ols_bs_res_gk.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_bs_gk_resid = ols_bs_res_gk.resid_pearson\n",
    "ols_bs_gk_fv = ols_bs_res_gk.fittedvalues\n",
    "\n",
    "gof_plots(y_train_gk_boxcox, ols_bs_gk_resid, ols_bs_gk_fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c075c",
   "metadata": {},
   "source": [
    "The above diagnostic plots show that there has been an improvement in the specification of the model as a result of the transformation of the response variable and the application of the backward elimination variable selection process. The QQ plot indicates that the standardised residuals more closely fit the standard normal distribution; the plot of standardised residuals vs fitted values shows a decrease in clear heteroskedasticity in the residuals; and, although the fitted values vs response plot shows an increase in variablility, this is likely due to overfitting in the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771dd966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform an inverse Box-Cox transformation\n",
    "def inverse_boxcox(x, lmbda):\n",
    "    if lmbda == 0:\n",
    "        return(np.exp(x))\n",
    "    \n",
    "    else:\n",
    "        return((x*lmbda + 1)**(1/lmbda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the backward selection model to the test data\n",
    "X_test_gk_bs = X_test_gk1[X_train_gk_bs.columns]\n",
    "\n",
    "# Apply inverse Box-Cox transformation to the predicted values\n",
    "ols_bs_gk_test_predict_boxcox = ols_bs_res_gk.predict(X_test_gk_bs)\n",
    "ols_bs_gk_test_predict = inverse_boxcox(ols_bs_gk_test_predict_boxcox, lmbda=lambda_gk) - 1\n",
    "\n",
    "ols_bs_gk_test_resid = ols_bs_gk_test_predict - y_test_gk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6522f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_gk, ols_bs_gk_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ols_bs_gk_test_predict, ols_bs_gk_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "mae_bs = np.mean(abs(ols_bs_gk_test_resid))\n",
    "mse_bs = np.mean(ols_bs_gk_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(mae_bs, mse_bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150aabb",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "We now move on to fitting penalised regressions in order to select important variables. We begin by performing a cross-validation to determine the optimal order of magnitude for the penalisation parameter. For both the lasso and ridge regressions we continue to fit the linear model to the response after having applied the Box-Cox transform as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d651c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a lasso regression implementing 10-fold cross-validation for the selection of the tuning parameter\n",
    "validation_idxs = split(X_train_gk_boxcox.sample(frac=1).index, 10)\n",
    "lasso_gk_alpha_list = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "lasso_gk_cv_error_dict = {'mae': [], 'mse': []}\n",
    "for a in lasso_gk_alpha_list:\n",
    "    mae = []\n",
    "    mse = []\n",
    "    \n",
    "    for i, idx in enumerate(validation_idxs):\n",
    "        X_val_gk_cv = X_train_gk_boxcox.loc[idx]\n",
    "        y_val_gk_cv = y_train_gk[idx]\n",
    "        X_train_gk_cv = X_train_gk_boxcox.drop(idx, axis=0)\n",
    "        y_train_gk_cv = y_train_gk_boxcox.drop(idx)\n",
    "        \n",
    "        lasso_gk_model = sm.OLS(y_train_gk_cv, X_train_gk_cv)\n",
    "        lasso_gk_res = lasso_gk_model.fit_regularized(alpha=a, L1_wt=1)\n",
    "        \n",
    "        lasso_gk_cv_predict_boxcox = lasso_gk_res.predict(X_val_gk_cv)\n",
    "        lasso_gk_cv_predict = inverse_boxcox(lasso_gk_cv_predict_boxcox, lmbda=lambda_gk) - 1\n",
    "        \n",
    "        mae.append(np.mean(abs(lasso_gk_cv_predict - y_val_gk_cv)))\n",
    "        mse.append(np.mean((lasso_gk_cv_predict - y_val_gk_cv)**2))\n",
    "        \n",
    "    lasso_gk_cv_error_dict['mae'].append(np.mean(mae))\n",
    "    lasso_gk_cv_error_dict['mse'].append(np.mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MAE and MSE for different values of the tuning parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.plot(lasso_gk_alpha_list, lasso_gk_cv_error_dict['mae'])\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Alpha\")\n",
    "ax1.set_ylabel(\"Mean absolute error\")\n",
    "\n",
    "ax2.plot(lasso_gk_alpha_list, lasso_gk_cv_error_dict['mse'])\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Alpha\")\n",
    "ax2.set_ylabel(\"Mean square error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee9d2e",
   "metadata": {},
   "source": [
    "The plots above indicate that the best two options for the penalisation parameter, `alpha`, would be 0.1, which minimizes the mean absolute error, or 0.01, which minimizes the mean square error. Note that, here, the larger the size of the parameter the stricter the penalisation of the regression coefficients. I have chosen to use `alpha` = 0.01 as the minimizer of the mean square error on the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344495f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final lasso model\n",
    "lasso_gk_alpha = 1e-2\n",
    "\n",
    "lasso_gk_model = sm.OLS(y_train_gk_boxcox, X_train_gk_boxcox)\n",
    "lasso_gk_res = lasso_gk_model.fit_regularized(alpha=lasso_gk_alpha, L1_wt=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the response on the test set\n",
    "lasso_gk_test_predict_boxcox = lasso_gk_res.predict(X_test_gk2)\n",
    "lasso_gk_test_predict = inverse_boxcox(lasso_gk_test_predict_boxcox, lmbda=lambda_gk)\n",
    "lasso_gk_test_resid = lasso_gk_test_predict - y_test_gk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_gk, lasso_gk_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(lasso_gk_test_predict, lasso_gk_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "lasso_gk_mae = np.mean(abs(lasso_gk_test_resid))\n",
    "lasso_gk_mse = np.mean(lasso_gk_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(lasso_gk_mae, lasso_gk_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b542df",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a ridge regression implementing 10-fold cross-validation for the selection of the tuning parameter\n",
    "ridge_gk_alpha_list = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "ridge_gk_cv_error_dict = {'mae': [], 'mse': []}\n",
    "for a in ridge_gk_alpha_list:\n",
    "    mae = []\n",
    "    mse = []\n",
    "    \n",
    "    for i, idx in enumerate(validation_idxs):\n",
    "        X_val_gk_cv = X_train_gk_boxcox.loc[idx]\n",
    "        y_val_gk_cv = y_train_gk[idx]\n",
    "        X_train_gk_cv = X_train_gk_boxcox.drop(idx, axis=0)\n",
    "        y_train_gk_cv = y_train_gk_boxcox.drop(idx)\n",
    "        \n",
    "        ridge_gk_model = sm.OLS(y_train_gk_cv, X_train_gk_cv)\n",
    "        ridge_gk_res = ridge_gk_model.fit_regularized(alpha=a, L1_wt=0)\n",
    "        \n",
    "        ridge_gk_cv_predict_boxcox = ridge_gk_res.predict(X_val_gk_cv)\n",
    "        ridge_gk_cv_predict = inverse_boxcox(ridge_gk_cv_predict_boxcox, lmbda=lambda_gk) - 1\n",
    "        \n",
    "        mae.append(np.mean(abs(ridge_gk_cv_predict - y_val_gk_cv)))\n",
    "        mse.append(np.mean((ridge_gk_cv_predict - y_val_gk_cv)**2))\n",
    "        \n",
    "    ridge_gk_cv_error_dict['mae'].append(np.mean(mae))\n",
    "    ridge_gk_cv_error_dict['mse'].append(np.mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MAE and MSE on the validation sets for different values of the tuning parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.plot(ridge_gk_alpha_list, ridge_gk_cv_error_dict['mae'])\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Alpha\")\n",
    "ax1.set_ylabel(\"Mean absolute error\")\n",
    "\n",
    "ax2.plot(ridge_gk_alpha_list, ridge_gk_cv_error_dict['mse'])\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Alpha\")\n",
    "ax2.set_ylabel(\"Mean square error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7237c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final ridge model\n",
    "ridge_gk_alpha = 1e-1\n",
    "\n",
    "ridge_gk_model = sm.OLS(y_train_gk_boxcox, X_train_gk_boxcox)\n",
    "ridge_gk_res = ridge_gk_model.fit_regularized(alpha=ridge_gk_alpha, L1_wt=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c48514",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_gk_res.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the response on the test set\n",
    "ridge_gk_test_predict_boxcox = ridge_gk_res.predict(X_test_gk2)\n",
    "ridge_gk_test_predict = inverse_boxcox(ridge_gk_test_predict_boxcox, lmbda=lambda_gk)\n",
    "ridge_gk_test_resid = ridge_gk_test_predict - y_test_gk\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_gk, ridge_gk_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ridge_gk_test_predict, ridge_gk_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "ridge_gk_mae = np.mean(abs(ridge_gk_test_resid))\n",
    "ridge_gk_mse = np.mean(ridge_gk_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(ridge_gk_mae, ridge_gk_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c3c3f",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884725e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct 10-fold cross-validation on the parameter grid to select hyperparameters for the random forest\n",
    "# rf_gk_param_grid = {'max_depth': [1, 2, 3, 4, 5, 6, 10],\n",
    "#                     'n_estimators': [50, 100, 200, 500, 1000],\n",
    "#                     'max_features': [None, 'log2', 'sqrt']}\n",
    "rf_gk_param_grid = {'max_depth': [1, 2],\n",
    "                    'n_estimators': [50, 100],\n",
    "                    'max_features': [None, 'log2', 'sqrt']}\n",
    "scoring_metrics = ['neg_mean_absolute_error', 'neg_mean_squared_error']\n",
    "rf_gk_cv_results = GridSearchCV(RandomForestRegressor(), \n",
    "                                rf_gk_param_grid, \n",
    "                                scoring=scoring_metrics,\n",
    "                                cv=10,\n",
    "                                refit=False)\n",
    "rf_gk_cv_results.fit(X_train_gk1, y_train_gk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = rf_gk_cv_results.cv_results_\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "fig.subplots_adjust(hspace=0.3, top=0.92)\n",
    "\n",
    "for i, max_features in enumerate(rf_gk_param_grid['max_features']):\n",
    "    for n in rf_gk_param_grid['n_estimators']:\n",
    "        data_idx_bool = ((cv_results['param_max_features'] == max_features) & (cv_results['param_n_estimators'] == n))\n",
    "        depth_data = cv_results['param_max_depth'][data_idx_bool]\n",
    "        \n",
    "        mae_data = -1*cv_results['mean_test_neg_mean_absolute_error'][data_idx_bool]\n",
    "        # mae_error_data = cv_results['std_test_neg_mean_absolute_error'][data_idx_bool]\n",
    "        \n",
    "        mse_data = -1*cv_results['mean_test_neg_mean_squared_error'][data_idx_bool]\n",
    "        # mse_error_data = cv_results['std_test_neg_mean_squared_error'][data_idx_bool]\n",
    "        \n",
    "        axs[i, 0].plot(depth_data, mae_data)\n",
    "        axs[i, 1].plot(depth_data, mse_data)\n",
    "        \n",
    "        axs[i, 0].set_ylabel(\"Mean absolute error\")\n",
    "        axs[i, 1].set_ylabel(\"Mean squared error\")\n",
    "    \n",
    "axs[0, 0].set_xlabel(\"Max depth\")\n",
    "axs[0, 1].set_xlabel(\"Max depth\")\n",
    "\n",
    "# Add title and subtitles to figure\n",
    "fig.text(0.5, 0.94, \n",
    "         f\"Max features: '{rf_gk_param_grid['max_features'][0]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "fig.text(0.5, 0.65, \n",
    "         f\"Max features: '{rf_gk_param_grid['max_features'][1]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "fig.text(0.5, 0.355, \n",
    "         f\"Max features: '{rf_gk_param_grid['max_features'][2]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "# for n in n_estimators_list:\n",
    "#     axs[0].plot(max_depth, rf_gk_cv_mae[n]['train'], label=\"{}\".format(n))\n",
    "#     axs[1].plot(max_depth, rf_gk_cv_mse[n]['train'], label=\"{}\".format(n))\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.set_xscale('log')\n",
    "#     ax.set_xlabel('Maximum tree depth')\n",
    "#     ax.xaxis.set_minor_formatter(FormatStrFormatter(\"%.0f\"))\n",
    "#     ax.tick_params(which='major', labelbottom=False)\n",
    "\n",
    "# axs[0].set_ylabel('MAE')\n",
    "# axs[1].set_ylabel('MSE')\n",
    "# axs[1].legend()\n",
    "\n",
    "# fig.suptitle(\"Random forest training set performance\", fontsize=14)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# axes[2].plot(max_depth, rf_gk_cv_mae[n]['val'], label=\"{}\".format(n))\n",
    "# axes[3].plot(max_depth, rf_gk_cv_mse[n]['val'], label=\"{}\".format(n))\n",
    "\n",
    "# axes[2].set_ylabel('MAE on validation sets')\n",
    "# axes[3].set_ylabel('MSE on validation sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b129ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the random forest regression model with hyperparameters as determined above\n",
    "# max_depth_final = 4\n",
    "# n_estimators_final = 1000\n",
    "# rf_gk_model1 = RandomForestRegressor(max_depth=max_depth_final, random_state=0, oob_score=True, n_estimators=n_estimators_final)\n",
    "# rf_gk_model1.fit(X_train_gk1, y_train_gk)\n",
    "\n",
    "# rf1_gk_predict_train = rf_gk_model1.predict(X_train_gk1)\n",
    "# rf1_gk_predict_test = rf_gk_model1.predict(X_test_gk1)\n",
    "\n",
    "# rf1_gk_resid_train = rf1_gk_predict_train - y_train_gk\n",
    "# rf1_gk_resid_test = rf1_gk_predict_test - y_test_gk\n",
    "\n",
    "# rf1_gk_train_mae = np.mean(abs(resid_train))\n",
    "# rf1_gk_test_mae = np.mean(abs(resid_test))\n",
    "# rf1_gk_train_mse = np.mean(resid_train**2)\n",
    "# rf1_gk_test_mse = np.mean(resid_test**2)\n",
    "\n",
    "# print(\"MAE on test set = {}\\nMSE on test set = {}\".format(rf1_gk_test_mae, rf1_gk_test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815023c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine feature importance\n",
    "# feature_importances = (pd.Series(rf_gk_model1.feature_importances_, index=X_train_gk1.columns)\n",
    "#                        .sort_values()\n",
    "#                        .to_frame(name='importance'))\n",
    "# feature_importances['rank'] = feature_importances.importance.rank(ascending=False)\n",
    "# fig, ax = plt.subplots(figsize=(8, 52))\n",
    "# ax.margins(y = 0.0025)\n",
    "# ax.barh(feature_importances.index, feature_importances.importance)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_threshold = 0.005\n",
    "# rf_vars_initial = feature_importances.loc[feature_importances.importance >= importance_threshold].index.tolist()\n",
    "# n_vars = len(rf_vars_initial)\n",
    "# print(\"No. of variables = {}\".format(len(feature_importances)))\n",
    "# print(\"No. of variables above importance threshold = {}\".format(len(rf_vars_initial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033836db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We will only perform the variable selection CV using variables that are above the importance threshold in the initial model\n",
    "# # since running this for all variables would be time-consuming although theoretically preferred.\n",
    "\n",
    "# # Set up the variable selection cross-validation\n",
    "# validation_sets = split(X_train_gk1.index, 10)\n",
    "# max_depth = 4\n",
    "# n_estimators_list = [1000]\n",
    "# cv_info = {n: {} for n in n_estimators_list}\n",
    "\n",
    "# for n_estimators in n_estimators_list:\n",
    "#     Z_train = X_train_gk1[rf_vars_initial]\n",
    "#     Z_test = X_test_gk1[rf_vars_initial]\n",
    "#     rf_vars_current = rf_vars_initial.copy()\n",
    "#     cv_info[n_estimators] = {x: {} for x in range(1, len(rf_vars_initial)+1)}\n",
    "\n",
    "#     while len(rf_vars_current) > 0: \n",
    "#         print(len(rf_vars_current))\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['vars'] = rf_vars_current.copy()\n",
    "\n",
    "#         Z_train = Z_train[rf_vars_current]\n",
    "#         rf_regr_vs = RandomForestRegressor(max_depth=max_depth,\n",
    "#                                             random_state=0,\n",
    "#                                             oob_score=True,\n",
    "#                                             n_estimators=n_estimators)\n",
    "#         rf_regr_vs.fit(Z_train, y_train_gk)\n",
    "\n",
    "#         # Get MAE and MSE estimates using cross-validation\n",
    "#         mae = []\n",
    "#         mse = []\n",
    "#         for validation_idxs in validation_sets:\n",
    "#             Z_train1 = Z_train.drop(validation_idxs, axis=0)\n",
    "#             y_train_gk1 = y_train_gk.drop(validation_idxs)\n",
    "#             Z_val = Z_train.loc[validation_idxs]\n",
    "#             y_val_gk = y_train_gk.loc[validation_idxs]\n",
    "\n",
    "#             rf_regr = RandomForestRegressor(max_depth=max_depth,\n",
    "#                                             random_state=0,\n",
    "#                                             oob_score=True,\n",
    "#                                             n_estimators=n_estimators)\n",
    "#             rf_regr.fit(Z_train1, y_train_gk1)\n",
    "\n",
    "#             predict_val = rf_regr.predict(Z_val)\n",
    "#             resid_val = predict_val - y_val_gk\n",
    "#             mae.append(np.mean(abs(resid_val)))\n",
    "#             mse.append(np.mean(resid_val**2))\n",
    "\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['mae'] = np.mean(mae)\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['mse'] = np.mean(mse)\n",
    "\n",
    "#         var_importances = pd.Series(rf_regr_vs.feature_importances_, index=rf_vars_current).sort_values()\n",
    "#         drop_var = var_importances.index[0]\n",
    "#         rf_vars_current.remove(var_importances.index[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.linspace(1, n_vars, n_vars)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# for n_estimators in n_estimators_list:\n",
    "#     mae_varselect_cv = [cv_info[n_estimators][x]['mae'] for x in cv_info[n_estimators]]\n",
    "#     mse_varselect_cv = [cv_info[n_estimators][x]['mse'] for x in cv_info[n_estimators]]\n",
    "    \n",
    "#     ax1.plot(idx, mae_varselect_cv, label=\"{}\".format(n_estimators))\n",
    "#     ax2.plot(idx, mse_varselect_cv, label=\"{}\".format(n_estimators))\n",
    "    \n",
    "# ax1.set_xlabel(\"No. of variables\")\n",
    "# ax1.set_ylabel(\"Validation MAE\")\n",
    "# ax1.legend()\n",
    "# ax2.set_xlabel(\"No. of variables\")\n",
    "# ax2.set_ylabel(\"Validation MSE\")\n",
    "# ax2.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the model after selecting variables\n",
    "# n_estimators_final = 1000\n",
    "# n_vars_final = 22\n",
    "# gk_rf_vars_final = cv_info[n_estimators_final][n_vars_final]['vars']\n",
    "# X_train_gk_rf = X_train_gk1[gk_rf_vars_final]\n",
    "# X_test_gk_rf = X_test_gk1[gk_rf_vars_final]\n",
    "\n",
    "# rf_gk_model2 = RandomForestRegressor(max_depth=4, oob_score=True, n_estimators=n_estimators_final)\n",
    "# rf_gk_model2.fit(X_train_gk_rf, y_train_gk)\n",
    "\n",
    "# rf2_gk_test_predict = rf_gk_model2.predict(X_test_gk_rf)\n",
    "# rf2_gk_test_resid = rf2_gk_test_predict - y_test_gk\n",
    "# rf2_gk_test_mae = np.mean(abs(rf2_gk_test_resid))\n",
    "# rf2_gk_test_mse = np.mean(rf2_gk_test_resid**2)\n",
    "# print(\"MAE on test set = {:.3f}\\nMSE on test set = {:.3f}\".format(rf2_gk_test_mae, rf2_gk_test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8608506",
   "metadata": {},
   "source": [
    "# Defender modelling\n",
    "\n",
    "The process for modelling the points scored by defenders will be the same as for the goalkeepers. In order to avoid labouring points, I will mostly go through the process with little explanation, commenting only where instructive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ad59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated columns\n",
    "drop_cols = ([\"opp_npxGA_{}\".format(x) for x in range(1, 5)] +\n",
    "             [\"opp_npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"team_npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"team_npxGA_{}\".format(x) for x in range(1, 5)] +\n",
    "             [\"selected_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"value_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"creativity_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"xGBuildup_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"npg_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"starts_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"bps_{}\".format(x) for x in range(1, 5)]\n",
    "            )\n",
    "\n",
    "X_train_def2 = X_train_def1.drop(drop_cols, axis=1)\n",
    "X_test_def2 = X_test_def1.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6bfb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = X_train_def2.corr()\n",
    "\n",
    "corr_threshold = 0.85\n",
    "corr_pairs = []\n",
    "is_corr = (corr_mat >= corr_threshold)\n",
    "for i in range(is_corr.shape[0]):\n",
    "    for j in range(i+1, is_corr.shape[1]):\n",
    "        if is_corr.iloc[i, j]:\n",
    "            corr_pairs.append([\"{}\".format(is_corr.index[i]), \"{}\".format(is_corr.columns[j])])\n",
    "            \n",
    "corr_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52811d84",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44a12a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit an initial linear model to the training data\n",
    "ols_model_def = sm.OLS(y_train_def, X_train_def2)\n",
    "ols_res_def = ols_model_def.fit()\n",
    "\n",
    "ols_def_resid = ols_res_def.resid_pearson\n",
    "ols_def_fv = ols_res_def.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe20d3",
   "metadata": {},
   "source": [
    "The QQ plot of standardised residuals vs the theoretical quantiles appears to show significant skew in the distribution of residuals. Much like the plot for the goalkeepers, the plot of standardised residuals appears to indicate heteroskedasticity in the standardised residuals, with smaller variance in residuals for lower fitted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26512884",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gof_plots(y_train_def, ols_def_resid, ols_def_fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da6b69",
   "metadata": {},
   "source": [
    "I now want to apply the model to the test data in order to assess its performance. As a baseline model I will use the average value from the training data and assign this as the predicted value for all test set data. Then, we can compare the performance of future models against this uninformed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test MAE and MSE under the use of the training mean as a constant model\n",
    "def_baseline_mae = np.mean(abs(y_train_def.mean() - y_test_def))\n",
    "def_baseline_mse = np.mean((y_train_def.mean() - y_test_def)**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(def_baseline_mae, def_baseline_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b91e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted values for the test data\n",
    "X_test_def2 = X_test_def1[X_train_def2.columns]\n",
    "ols_def_test_predict = ols_res_def.predict(X_test_def2)\n",
    "ols_def_test_resid = ols_def_test_predict - y_test_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f903f87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_def, ols_def_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ols_def_test_predict, ols_def_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test MAE and MSE\n",
    "ols_def_test_mae = np.mean(abs(ols_def_test_resid))\n",
    "ols_def_test_mse = np.mean(ols_def_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(ols_def_test_mae, ols_def_test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 1\n",
    "y_train_def_boxcox = y_train_def.drop(y_train_def.loc[y_train_def < 0].index) + shift\n",
    "y_train_def_boxcox_index = y_train_def_boxcox.index\n",
    "X_train_def_boxcox = X_train_def2.drop(y_train_def.loc[y_train_def < 0].index)\n",
    "\n",
    "y_train_def_boxcox, lambda_def = stats.boxcox(y_train_def_boxcox)\n",
    "\n",
    "y_train_def_boxcox = pd.Series(y_train_def_boxcox, y_train_def_boxcox_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stepwise backward elimination, removing variables whose coefficient estimates do not have p-values falling below\n",
    "# the specified threshold\n",
    "priority_list_def = priority_list.copy()\n",
    "dropped_cols = []\n",
    "X_train_def_temp = X_train_def_boxcox.copy()\n",
    "selection_complete = False\n",
    "p_value_threshold = 0.1\n",
    "\n",
    "while not selection_complete:\n",
    "    model_temp = sm.OLS(y_train_def_boxcox, X_train_def_temp)\n",
    "    res_temp = model_temp.fit()\n",
    "    drop_candidates = res_temp.pvalues.loc[res_temp.pvalues > p_value_threshold]\n",
    "    if len(drop_candidates) == 0:\n",
    "        selection_complete = True\n",
    "        continue\n",
    "    \n",
    "    drop_col = priority_list_def.pop([col in drop_candidates for col in priority_list_def].index(True))\n",
    "    dropped_cols.append(drop_col)\n",
    "    X_train_def_temp.drop(drop_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine model chosen by backward elimination\n",
    "X_train_def_bs = sm.add_constant(X_train_def_temp)\n",
    "ols_bs_model_def = sm.OLS(y_train_def_boxcox, X_train_def_bs)\n",
    "ols_bs_res_def = ols_bs_model_def.fit()\n",
    "\n",
    "print(ols_bs_res_def.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631090e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_bs_def_resid = ols_bs_res_def.resid_pearson\n",
    "ols_bs_def_fv = ols_bs_res_def.fittedvalues\n",
    "\n",
    "gof_plots(y_train_def_boxcox, ols_bs_def_resid, ols_bs_def_fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec16459",
   "metadata": {},
   "source": [
    "The above diagnostic plots show that there has been an improvement in the specification of the model as a result of the transformation of the response variable and the application of the backward elimination variable selection process. The QQ plot indicates that the standardised residuals more closely fit the standard normal distribution; the plot of standardised residuals vs fitted values shows a decrease in clear heteroskedasticity in the residuals; and, although the fitted values vs response plot shows an increase in variablility, this is likely due to overfitting in the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aae556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the backward selection model to the test data\n",
    "X_test_def_bs = X_test_def1[X_train_def_bs.columns]\n",
    "\n",
    "# Apply inverse Box-Cox transformation to the predicted values\n",
    "ols_bs_def_test_predict_boxcox = ols_bs_res_def.predict(X_test_def_bs)\n",
    "ols_bs_def_test_predict = inverse_boxcox(ols_bs_def_test_predict_boxcox, lmbda=lambda_def) - 1\n",
    "\n",
    "ols_bs_def_test_resid = ols_bs_def_test_predict - y_test_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c668bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_def, ols_bs_def_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ols_bs_def_test_predict, ols_bs_def_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f72cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "mae_bs = np.mean(abs(ols_bs_def_test_resid))\n",
    "mse_bs = np.mean(ols_bs_def_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(mae_bs, mse_bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53bc3f6",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "We now move on to fitting penalised regressions in order to select important variables. We begin by performing a cross-validation to determine the optimal order of magnitude for the penalisation parameter. For both the lasso and ridge regressions we continue to fit the linear model to the response after having applied the Box-Cox transform as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be537f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a lasso regression implementing 10-fold cross-validation for the selection of the tuning parameter\n",
    "validation_idxs = split(X_train_def_boxcox.sample(frac=1).index, 10)\n",
    "lasso_def_alpha_list = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "lasso_def_cv_error_dict = {'mae': [], 'mse': []}\n",
    "for a in lasso_def_alpha_list:\n",
    "    mae = []\n",
    "    mse = []\n",
    "    \n",
    "    for i, idx in enumerate(validation_idxs):\n",
    "        X_val_def_cv = X_train_def_boxcox.loc[idx]\n",
    "        y_val_def_cv = y_train_def[idx]\n",
    "        X_train_def_cv = X_train_def_boxcox.drop(idx, axis=0)\n",
    "        y_train_def_cv = y_train_def_boxcox.drop(idx)\n",
    "        \n",
    "        lasso_def_model = sm.OLS(y_train_def_cv, X_train_def_cv)\n",
    "        lasso_def_res = lasso_def_model.fit_regularized(alpha=a, L1_wt=1)\n",
    "        \n",
    "        lasso_def_cv_predict_boxcox = lasso_def_res.predict(X_val_def_cv)\n",
    "        lasso_def_cv_predict = inverse_boxcox(lasso_def_cv_predict_boxcox, lmbda=lambda_def) - 1\n",
    "        \n",
    "        mae.append(np.mean(abs(lasso_def_cv_predict - y_val_def_cv)))\n",
    "        mse.append(np.mean((lasso_def_cv_predict - y_val_def_cv)**2))\n",
    "        \n",
    "    lasso_def_cv_error_dict['mae'].append(np.mean(mae))\n",
    "    lasso_def_cv_error_dict['mse'].append(np.mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e23c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MAE and MSE for different values of the tuning parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.plot(lasso_def_alpha_list, lasso_def_cv_error_dict['mae'])\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Alpha\")\n",
    "ax1.set_ylabel(\"Mean absolute error\")\n",
    "\n",
    "ax2.plot(lasso_def_alpha_list, lasso_def_cv_error_dict['mse'])\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Alpha\")\n",
    "ax2.set_ylabel(\"Mean square error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3391161b",
   "metadata": {},
   "source": [
    "The plots above indicate that the best two options for the penalisation parameter, `alpha`, would be 0.1, which minimizes the mean absolute error, or 0.01, which minimizes the mean square error. Note that, here, the larger the size of the parameter the stricter the penalisation of the regression coefficients. I have chosen to use `alpha` = 0.01 as the minimizer of the mean square error on the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final lasso model\n",
    "lasso_def_alpha = 1e-2\n",
    "\n",
    "lasso_def_model = sm.OLS(y_train_def_boxcox, X_train_def_boxcox)\n",
    "lasso_def_res = lasso_def_model.fit_regularized(alpha=lasso_def_alpha, L1_wt=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the response on the test set\n",
    "lasso_def_test_predict_boxcox = lasso_def_res.predict(X_test_def2)\n",
    "lasso_def_test_predict = inverse_boxcox(lasso_def_test_predict_boxcox, lmbda=lambda_def)\n",
    "lasso_def_test_resid = lasso_def_test_predict - y_test_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_def, lasso_def_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(lasso_def_test_predict, lasso_def_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1929561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "lasso_def_mae = np.mean(abs(lasso_def_test_resid))\n",
    "lasso_def_mse = np.mean(lasso_def_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(lasso_def_mae, lasso_def_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e0fa1",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c760cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a ridge regression implementing 10-fold cross-validation for the selection of the tuning parameter\n",
    "ridge_def_alpha_list = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "ridge_def_cv_error_dict = {'mae': [], 'mse': []}\n",
    "for a in ridge_def_alpha_list:\n",
    "    mae = []\n",
    "    mse = []\n",
    "    \n",
    "    for i, idx in enumerate(validation_idxs):\n",
    "        X_val_def_cv = X_train_def_boxcox.loc[idx]\n",
    "        y_val_def_cv = y_train_def[idx]\n",
    "        X_train_def_cv = X_train_def_boxcox.drop(idx, axis=0)\n",
    "        y_train_def_cv = y_train_def_boxcox.drop(idx)\n",
    "        \n",
    "        ridge_def_model = sm.OLS(y_train_def_cv, X_train_def_cv)\n",
    "        ridge_def_res = ridge_def_model.fit_regularized(alpha=a, L1_wt=0)\n",
    "        \n",
    "        ridge_def_cv_predict_boxcox = ridge_def_res.predict(X_val_def_cv)\n",
    "        ridge_def_cv_predict = inverse_boxcox(ridge_def_cv_predict_boxcox, lmbda=lambda_def) - 1\n",
    "        \n",
    "        mae.append(np.mean(abs(ridge_def_cv_predict - y_val_def_cv)))\n",
    "        mse.append(np.mean((ridge_def_cv_predict - y_val_def_cv)**2))\n",
    "        \n",
    "    ridge_def_cv_error_dict['mae'].append(np.mean(mae))\n",
    "    ridge_def_cv_error_dict['mse'].append(np.mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce58400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MAE and MSE on the validation sets for different values of the tuning parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.plot(ridge_def_alpha_list, ridge_def_cv_error_dict['mae'])\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Alpha\")\n",
    "ax1.set_ylabel(\"Mean absolute error\")\n",
    "\n",
    "ax2.plot(ridge_def_alpha_list, ridge_def_cv_error_dict['mse'])\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Alpha\")\n",
    "ax2.set_ylabel(\"Mean square error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final ridge model\n",
    "ridge_def_alpha = 1e-1\n",
    "\n",
    "ridge_def_model = sm.OLS(y_train_def_boxcox, X_train_def_boxcox)\n",
    "ridge_def_res = ridge_def_model.fit_regularized(alpha=ridge_def_alpha, L1_wt=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the response on the test set\n",
    "ridge_def_test_predict_boxcox = ridge_def_res.predict(X_test_def2)\n",
    "ridge_def_test_predict = inverse_boxcox(ridge_def_test_predict_boxcox, lmbda=lambda_def)\n",
    "ridge_def_test_resid = ridge_def_test_predict - y_test_def\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_def, ridge_def_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ridge_def_test_predict, ridge_def_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "ridge_def_mae = np.mean(abs(ridge_def_test_resid))\n",
    "ridge_def_mse = np.mean(ridge_def_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(ridge_def_mae, ridge_def_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0b8ad",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9055ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct 10-fold cross-validation on the parameter grid to select hyperparameters for the random forest\n",
    "# rf_def_param_grid = {'max_depth': [1, 2, 3, 4, 5, 6, 10],\n",
    "#                     'n_estimators': [50, 100, 200, 500, 1000],\n",
    "#                     'max_features': [None, 'log2', 'sqrt']}\n",
    "rf_def_param_grid = {'max_depth': [1, 2],\n",
    "                    'n_estimators': [50, 100],\n",
    "                    'max_features': [None, 'log2', 'sqrt']}\n",
    "scoring_metrics = ['neg_mean_absolute_error', 'neg_mean_squared_error']\n",
    "rf_def_cv_results = GridSearchCV(RandomForestRegressor(), \n",
    "                                rf_def_param_grid, \n",
    "                                scoring=scoring_metrics,\n",
    "                                cv=10,\n",
    "                                refit=False)\n",
    "rf_def_cv_results.fit(X_train_def1, y_train_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = rf_def_cv_results.cv_results_\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "fig.subplots_adjust(hspace=0.3, top=0.92)\n",
    "\n",
    "for i, max_features in enumerate(rf_def_param_grid['max_features']):\n",
    "    for n in rf_def_param_grid['n_estimators']:\n",
    "        data_idx_bool = ((cv_results['param_max_features'] == max_features) & (cv_results['param_n_estimators'] == n))\n",
    "        depth_data = cv_results['param_max_depth'][data_idx_bool]\n",
    "        \n",
    "        mae_data = -1*cv_results['mean_test_neg_mean_absolute_error'][data_idx_bool]\n",
    "        # mae_error_data = cv_results['std_test_neg_mean_absolute_error'][data_idx_bool]\n",
    "        \n",
    "        mse_data = -1*cv_results['mean_test_neg_mean_squared_error'][data_idx_bool]\n",
    "        # mse_error_data = cv_results['std_test_neg_mean_squared_error'][data_idx_bool]\n",
    "        \n",
    "        axs[i, 0].plot(depth_data, mae_data)\n",
    "        axs[i, 1].plot(depth_data, mse_data)\n",
    "        \n",
    "        axs[i, 0].set_ylabel(\"Mean absolute error\")\n",
    "        axs[i, 1].set_ylabel(\"Mean squared error\")\n",
    "    \n",
    "axs[0, 0].set_xlabel(\"Max depth\")\n",
    "axs[0, 1].set_xlabel(\"Max depth\")\n",
    "\n",
    "# Add title and subtitles to figure\n",
    "fig.text(0.5, 0.94, \n",
    "         f\"Max features: '{rf_def_param_grid['max_features'][0]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "fig.text(0.5, 0.65, \n",
    "         f\"Max features: '{rf_def_param_grid['max_features'][1]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "fig.text(0.5, 0.355, \n",
    "         f\"Max features: '{rf_def_param_grid['max_features'][2]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "# for n in n_estimators_list:\n",
    "#     axs[0].plot(max_depth, rf_def_cv_mae[n]['train'], label=\"{}\".format(n))\n",
    "#     axs[1].plot(max_depth, rf_def_cv_mse[n]['train'], label=\"{}\".format(n))\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.set_xscale('log')\n",
    "#     ax.set_xlabel('Maximum tree depth')\n",
    "#     ax.xaxis.set_minor_formatter(FormatStrFormatter(\"%.0f\"))\n",
    "#     ax.tick_params(which='major', labelbottom=False)\n",
    "\n",
    "# axs[0].set_ylabel('MAE')\n",
    "# axs[1].set_ylabel('MSE')\n",
    "# axs[1].legend()\n",
    "\n",
    "# fig.suptitle(\"Random forest training set performance\", fontsize=14)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# axes[2].plot(max_depth, rf_def_cv_mae[n]['val'], label=\"{}\".format(n))\n",
    "# axes[3].plot(max_depth, rf_def_cv_mse[n]['val'], label=\"{}\".format(n))\n",
    "\n",
    "# axes[2].set_ylabel('MAE on validation sets')\n",
    "# axes[3].set_ylabel('MSE on validation sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cdcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the random forest regression model with hyperparameters as determined above\n",
    "# max_depth_final = 4\n",
    "# n_estimators_final = 1000\n",
    "# rf_def_model1 = RandomForestRegressor(max_depth=max_depth_final, random_state=0, oob_score=True, n_estimators=n_estimators_final)\n",
    "# rf_def_model1.fit(X_train_def1, y_train_def)\n",
    "\n",
    "# rf1_def_predict_train = rf_def_model1.predict(X_train_def1)\n",
    "# rf1_def_predict_test = rf_def_model1.predict(X_test_def1)\n",
    "\n",
    "# rf1_def_resid_train = rf1_def_predict_train - y_train_def\n",
    "# rf1_def_resid_test = rf1_def_predict_test - y_test_def\n",
    "\n",
    "# rf1_def_train_mae = np.mean(abs(resid_train))\n",
    "# rf1_def_test_mae = np.mean(abs(resid_test))\n",
    "# rf1_def_train_mse = np.mean(resid_train**2)\n",
    "# rf1_def_test_mse = np.mean(resid_test**2)\n",
    "\n",
    "# print(\"MAE on test set = {}\\nMSE on test set = {}\".format(rf1_def_test_mae, rf1_def_test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine feature importance\n",
    "# feature_importances = (pd.Series(rf_def_model1.feature_importances_, index=X_train_def1.columns)\n",
    "#                        .sort_values()\n",
    "#                        .to_frame(name='importance'))\n",
    "# feature_importances['rank'] = feature_importances.importance.rank(ascending=False)\n",
    "# fig, ax = plt.subplots(figsize=(8, 52))\n",
    "# ax.margins(y = 0.0025)\n",
    "# ax.barh(feature_importances.index, feature_importances.importance)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_threshold = 0.005\n",
    "# rf_vars_initial = feature_importances.loc[feature_importances.importance >= importance_threshold].index.tolist()\n",
    "# n_vars = len(rf_vars_initial)\n",
    "# print(\"No. of variables = {}\".format(len(feature_importances)))\n",
    "# print(\"No. of variables above importance threshold = {}\".format(len(rf_vars_initial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We will only perform the variable selection CV using variables that are above the importance threshold in the initial model\n",
    "# # since running this for all variables would be time-consuming although theoretically preferred.\n",
    "\n",
    "# # Set up the variable selection cross-validation\n",
    "# validation_sets = split(X_train_def1.index, 10)\n",
    "# max_depth = 4\n",
    "# n_estimators_list = [1000]\n",
    "# cv_info = {n: {} for n in n_estimators_list}\n",
    "\n",
    "# for n_estimators in n_estimators_list:\n",
    "#     Z_train = X_train_def1[rf_vars_initial]\n",
    "#     Z_test = X_test_def1[rf_vars_initial]\n",
    "#     rf_vars_current = rf_vars_initial.copy()\n",
    "#     cv_info[n_estimators] = {x: {} for x in range(1, len(rf_vars_initial)+1)}\n",
    "\n",
    "#     while len(rf_vars_current) > 0: \n",
    "#         print(len(rf_vars_current))\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['vars'] = rf_vars_current.copy()\n",
    "\n",
    "#         Z_train = Z_train[rf_vars_current]\n",
    "#         rf_regr_vs = RandomForestRegressor(max_depth=max_depth,\n",
    "#                                             random_state=0,\n",
    "#                                             oob_score=True,\n",
    "#                                             n_estimators=n_estimators)\n",
    "#         rf_regr_vs.fit(Z_train, y_train_def)\n",
    "\n",
    "#         # Get MAE and MSE estimates using cross-validation\n",
    "#         mae = []\n",
    "#         mse = []\n",
    "#         for validation_idxs in validation_sets:\n",
    "#             Z_train1 = Z_train.drop(validation_idxs, axis=0)\n",
    "#             y_train_def1 = y_train_def.drop(validation_idxs)\n",
    "#             Z_val = Z_train.loc[validation_idxs]\n",
    "#             y_val_def = y_train_def.loc[validation_idxs]\n",
    "\n",
    "#             rf_regr = RandomForestRegressor(max_depth=max_depth,\n",
    "#                                             random_state=0,\n",
    "#                                             oob_score=True,\n",
    "#                                             n_estimators=n_estimators)\n",
    "#             rf_regr.fit(Z_train1, y_train_def1)\n",
    "\n",
    "#             predict_val = rf_regr.predict(Z_val)\n",
    "#             resid_val = predict_val - y_val_def\n",
    "#             mae.append(np.mean(abs(resid_val)))\n",
    "#             mse.append(np.mean(resid_val**2))\n",
    "\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['mae'] = np.mean(mae)\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['mse'] = np.mean(mse)\n",
    "\n",
    "#         var_importances = pd.Series(rf_regr_vs.feature_importances_, index=rf_vars_current).sort_values()\n",
    "#         drop_var = var_importances.index[0]\n",
    "#         rf_vars_current.remove(var_importances.index[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.linspace(1, n_vars, n_vars)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# for n_estimators in n_estimators_list:\n",
    "#     mae_varselect_cv = [cv_info[n_estimators][x]['mae'] for x in cv_info[n_estimators]]\n",
    "#     mse_varselect_cv = [cv_info[n_estimators][x]['mse'] for x in cv_info[n_estimators]]\n",
    "    \n",
    "#     ax1.plot(idx, mae_varselect_cv, label=\"{}\".format(n_estimators))\n",
    "#     ax2.plot(idx, mse_varselect_cv, label=\"{}\".format(n_estimators))\n",
    "    \n",
    "# ax1.set_xlabel(\"No. of variables\")\n",
    "# ax1.set_ylabel(\"Validation MAE\")\n",
    "# ax1.legend()\n",
    "# ax2.set_xlabel(\"No. of variables\")\n",
    "# ax2.set_ylabel(\"Validation MSE\")\n",
    "# ax2.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231180f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Fit the model after selecting variables\n",
    "# n_estimators_final = 1000\n",
    "# n_vars_final = 22\n",
    "# def_rf_vars_final = cv_info[n_estimators_final][n_vars_final]['vars']\n",
    "# X_train_def_rf = X_train_def1[def_rf_vars_final]\n",
    "# X_test_def_rf = X_test_def1[def_rf_vars_final]\n",
    "\n",
    "# rf_def_model2 = RandomForestRegressor(max_depth=4, oob_score=True, n_estimators=n_estimators_final)\n",
    "# rf_def_model2.fit(X_train_def_rf, y_train_def)\n",
    "\n",
    "# rf2_def_test_predict = rf_def_model2.predict(X_test_def_rf)\n",
    "# rf2_def_test_resid = rf2_def_test_predict - y_test_def\n",
    "# rf2_def_test_mae = np.mean(abs(rf2_def_test_resid))\n",
    "# rf2_def_test_mse = np.mean(rf2_def_test_resid**2)\n",
    "# print(\"MAE on test set = {:.3f}\\nMSE on test set = {:.3f}\".format(rf2_def_test_mae, rf2_def_test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0538485e",
   "metadata": {},
   "source": [
    "# Midfielder modelling\n",
    "\n",
    "The process for modelling the points scored by midfielders will be the same as for the goalkeepers. In order to avoid labouring points, I will mostly go through the process with little explanation, commenting only where instructive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated columns\n",
    "drop_cols = ([\"opp_npxGA_{}\".format(x) for x in range(1, 5)] +\n",
    "             [\"opp_npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"team_npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"team_npxGA_{}\".format(x) for x in range(1, 5)] +\n",
    "             [\"selected_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"value_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"creativity_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"influence_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"npg_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"starts_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"bps_{}\".format(x) for x in range(1, 5)]\n",
    "            )\n",
    "\n",
    "X_train_mid2 = X_train_mid1.drop(drop_cols, axis=1)\n",
    "X_test_mid2 = X_test_mid1.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afccfdc",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c6dec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit an initial linear model to the training data\n",
    "ols_model_mid = sm.OLS(y_train_mid, X_train_mid2)\n",
    "ols_res_mid = ols_model_mid.fit()\n",
    "\n",
    "ols_mid_resid = ols_res_mid.resid_pearson\n",
    "ols_mid_fv = ols_res_mid.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72943260",
   "metadata": {},
   "source": [
    "The QQ plot of standardised residuals vs the theoretical quantiles appears to show significant skew in the distribution of residuals. Much like the plot for the goalkeepers, the plot of standardised residuals appears to indicate heteroskedasticity in the standardised residuals, with smaller variance in residuals for lower fitted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f6ff0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gof_plots(y_train_mid, ols_mid_resid, ols_mid_fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3b05d",
   "metadata": {},
   "source": [
    "I now want to apply the model to the test data in order to assess its performance. As a baseline model I will use the average value from the training data and assign this as the predicted value for all test set data. Then, we can compare the performance of future models against this uninformed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a691bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test MAE and MSE under the use of the training mean as a constant model\n",
    "mid_baseline_mae = np.mean(abs(y_train_mid.mean() - y_test_mid))\n",
    "mid_baseline_mse = np.mean((y_train_mid.mean() - y_test_mid)**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(mid_baseline_mae, mid_baseline_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01aae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted values for the test data\n",
    "X_test_mid2 = X_test_mid1[X_train_mid2.columns]\n",
    "ols_mid_test_predict = ols_res_mid.predict(X_test_mid2)\n",
    "ols_mid_test_resid = ols_mid_test_predict - y_test_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01227bc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_mid, ols_mid_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ols_mid_test_predict, ols_mid_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test MAE and MSE\n",
    "ols_mid_test_mae = np.mean(abs(ols_mid_test_resid))\n",
    "ols_mid_test_mse = np.mean(ols_mid_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(ols_mid_test_mae, ols_mid_test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 1\n",
    "y_train_mid_boxcox = y_train_mid.drop(y_train_mid.loc[y_train_mid < 0].index) + shift\n",
    "y_train_mid_boxcox_index = y_train_mid_boxcox.index\n",
    "X_train_mid_boxcox = X_train_mid2.drop(y_train_mid.loc[y_train_mid < 0].index)\n",
    "\n",
    "y_train_mid_boxcox, lambda_mid = stats.boxcox(y_train_mid_boxcox)\n",
    "\n",
    "y_train_mid_boxcox = pd.Series(y_train_mid_boxcox, y_train_mid_boxcox_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d448bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stepwise backward elimination, removing variables whose coefficient estimates do not have p-values falling below\n",
    "# the specified threshold\n",
    "priority_list_mid = priority_list.copy()\n",
    "dropped_cols = []\n",
    "X_train_mid_temp = X_train_mid_boxcox.copy()\n",
    "selection_complete = False\n",
    "p_value_threshold = 0.1\n",
    "\n",
    "while not selection_complete:\n",
    "    model_temp = sm.OLS(y_train_mid_boxcox, X_train_mid_temp)\n",
    "    res_temp = model_temp.fit()\n",
    "    drop_candidates = res_temp.pvalues.loc[res_temp.pvalues > p_value_threshold]\n",
    "    if len(drop_candidates) == 0:\n",
    "        selection_complete = True\n",
    "        continue\n",
    "    \n",
    "    drop_col = priority_list_mid.pop([col in drop_candidates for col in priority_list_mid].index(True))\n",
    "    dropped_cols.append(drop_col)\n",
    "    X_train_mid_temp.drop(drop_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine model chosen by backward elimination\n",
    "X_train_mid_bs = sm.add_constant(X_train_mid_temp)\n",
    "ols_bs_model_mid = sm.OLS(y_train_mid_boxcox, X_train_mid_bs)\n",
    "ols_bs_res_mid = ols_bs_model_mid.fit()\n",
    "\n",
    "print(ols_bs_res_mid.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_bs_mid_resid = ols_bs_res_mid.resid_pearson\n",
    "ols_bs_mid_fv = ols_bs_res_mid.fittedvalues\n",
    "\n",
    "gof_plots(y_train_mid_boxcox, ols_bs_mid_resid, ols_bs_mid_fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab30e66",
   "metadata": {},
   "source": [
    "The above diagnostic plots show that there has been an improvement in the specification of the model as a result of the transformation of the response variable and the application of the backward elimination variable selection process. The QQ plot indicates that the standardised residuals more closely fit the standard normal distribution; the plot of standardised residuals vs fitted values shows a decrease in clear heteroskedasticity in the residuals; and, although the fitted values vs response plot shows an increase in variablility, this is likely due to overfitting in the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74016b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the backward selection model to the test data\n",
    "X_test_mid_bs = X_test_mid1[X_train_mid_bs.columns]\n",
    "\n",
    "# Apply inverse Box-Cox transformation to the predicted values\n",
    "ols_bs_mid_test_predict_boxcox = ols_bs_res_mid.predict(X_test_mid_bs)\n",
    "ols_bs_mid_test_predict = inverse_boxcox(ols_bs_mid_test_predict_boxcox, lmbda=lambda_mid) - 1\n",
    "\n",
    "ols_bs_mid_test_resid = ols_bs_mid_test_predict - y_test_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_mid, ols_bs_mid_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ols_bs_mid_test_predict, ols_bs_mid_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ddd313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "mae_bs = np.mean(abs(ols_bs_mid_test_resid))\n",
    "mse_bs = np.mean(ols_bs_mid_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(mae_bs, mse_bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39a03a",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "We now move on to fitting penalised regressions in order to select important variables. We begin by performing a cross-validation to determine the optimal order of magnitude for the penalisation parameter. For both the lasso and ridge regressions we continue to fit the linear model to the response after having applied the Box-Cox transform as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e912c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a lasso regression implementing 10-fold cross-validation for the selection of the tuning parameter\n",
    "validation_idxs = split(X_train_mid_boxcox.sample(frac=1).index, 10)\n",
    "lasso_mid_alpha_list = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "lasso_mid_cv_error_dict = {'mae': [], 'mse': []}\n",
    "for a in lasso_mid_alpha_list:\n",
    "    mae = []\n",
    "    mse = []\n",
    "    \n",
    "    for i, idx in enumerate(validation_idxs):\n",
    "        X_val_mid_cv = X_train_mid_boxcox.loc[idx]\n",
    "        y_val_mid_cv = y_train_mid[idx]\n",
    "        X_train_mid_cv = X_train_mid_boxcox.drop(idx, axis=0)\n",
    "        y_train_mid_cv = y_train_mid_boxcox.drop(idx)\n",
    "        \n",
    "        lasso_mid_model = sm.OLS(y_train_mid_cv, X_train_mid_cv)\n",
    "        lasso_mid_res = lasso_mid_model.fit_regularized(alpha=a, L1_wt=1)\n",
    "        \n",
    "        lasso_mid_cv_predict_boxcox = lasso_mid_res.predict(X_val_mid_cv)\n",
    "        lasso_mid_cv_predict = inverse_boxcox(lasso_mid_cv_predict_boxcox, lmbda=lambda_mid) - 1\n",
    "        \n",
    "        mae.append(np.mean(abs(lasso_mid_cv_predict - y_val_mid_cv)))\n",
    "        mse.append(np.mean((lasso_mid_cv_predict - y_val_mid_cv)**2))\n",
    "        \n",
    "    lasso_mid_cv_error_dict['mae'].append(np.mean(mae))\n",
    "    lasso_mid_cv_error_dict['mse'].append(np.mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MAE and MSE for different values of the tuning parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.plot(lasso_mid_alpha_list, lasso_mid_cv_error_dict['mae'])\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Alpha\")\n",
    "ax1.set_ylabel(\"Mean absolute error\")\n",
    "\n",
    "ax2.plot(lasso_mid_alpha_list, lasso_mid_cv_error_dict['mse'])\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Alpha\")\n",
    "ax2.set_ylabel(\"Mean square error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab0673",
   "metadata": {},
   "source": [
    "The plots above indicate that the best two options for the penalisation parameter, `alpha`, would be 0.1, which minimizes the mean absolute error, or 0.01, which minimizes the mean square error. Note that, here, the larger the size of the parameter the stricter the penalisation of the regression coefficients. I have chosen to use `alpha` = 0.01 as the minimizer of the mean square error on the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48295520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final lasso model\n",
    "lasso_mid_alpha = 1e-2\n",
    "\n",
    "lasso_mid_model = sm.OLS(y_train_mid_boxcox, X_train_mid_boxcox)\n",
    "lasso_mid_res = lasso_mid_model.fit_regularized(alpha=lasso_mid_alpha, L1_wt=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the response on the test set\n",
    "lasso_mid_test_predict_boxcox = lasso_mid_res.predict(X_test_mid2)\n",
    "lasso_mid_test_predict = inverse_boxcox(lasso_mid_test_predict_boxcox, lmbda=lambda_mid)\n",
    "lasso_mid_test_resid = lasso_mid_test_predict - y_test_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70aaad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_mid, lasso_mid_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(lasso_mid_test_predict, lasso_mid_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "lasso_mid_mae = np.mean(abs(lasso_mid_test_resid))\n",
    "lasso_mid_mse = np.mean(lasso_mid_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(lasso_mid_mae, lasso_mid_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78b706",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f68446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a ridge regression implementing 10-fold cross-validation for the selection of the tuning parameter\n",
    "ridge_mid_alpha_list = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "ridge_mid_cv_error_dict = {'mae': [], 'mse': []}\n",
    "for a in ridge_mid_alpha_list:\n",
    "    mae = []\n",
    "    mse = []\n",
    "    \n",
    "    for i, idx in enumerate(validation_idxs):\n",
    "        X_val_mid_cv = X_train_mid_boxcox.loc[idx]\n",
    "        y_val_mid_cv = y_train_mid[idx]\n",
    "        X_train_mid_cv = X_train_mid_boxcox.drop(idx, axis=0)\n",
    "        y_train_mid_cv = y_train_mid_boxcox.drop(idx)\n",
    "        \n",
    "        ridge_mid_model = sm.OLS(y_train_mid_cv, X_train_mid_cv)\n",
    "        ridge_mid_res = ridge_mid_model.fit_regularized(alpha=a, L1_wt=0)\n",
    "        \n",
    "        ridge_mid_cv_predict_boxcox = ridge_mid_res.predict(X_val_mid_cv)\n",
    "        ridge_mid_cv_predict = inverse_boxcox(ridge_mid_cv_predict_boxcox, lmbda=lambda_mid) - 1\n",
    "        \n",
    "        mae.append(np.mean(abs(ridge_mid_cv_predict - y_val_mid_cv)))\n",
    "        mse.append(np.mean((ridge_mid_cv_predict - y_val_mid_cv)**2))\n",
    "        \n",
    "    ridge_mid_cv_error_dict['mae'].append(np.mean(mae))\n",
    "    ridge_mid_cv_error_dict['mse'].append(np.mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc17f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MAE and MSE on the validation sets for different values of the tuning parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.plot(ridge_mid_alpha_list, ridge_mid_cv_error_dict['mae'])\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Alpha\")\n",
    "ax1.set_ylabel(\"Mean absolute error\")\n",
    "\n",
    "ax2.plot(ridge_mid_alpha_list, ridge_mid_cv_error_dict['mse'])\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Alpha\")\n",
    "ax2.set_ylabel(\"Mean square error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final ridge model\n",
    "ridge_mid_alpha = 1e-2\n",
    "\n",
    "ridge_mid_model = sm.OLS(y_train_mid_boxcox, X_train_mid_boxcox)\n",
    "ridge_mid_res = ridge_mid_model.fit_regularized(alpha=ridge_mid_alpha, L1_wt=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3806443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the response on the test set\n",
    "ridge_mid_test_predict_boxcox = ridge_mid_res.predict(X_test_mid2)\n",
    "ridge_mid_test_predict = inverse_boxcox(ridge_mid_test_predict_boxcox, lmbda=lambda_mid)\n",
    "ridge_mid_test_resid = ridge_mid_test_predict - y_test_mid\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_mid, ridge_mid_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ridge_mid_test_predict, ridge_mid_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "ridge_mid_mae = np.mean(abs(ridge_mid_test_resid))\n",
    "ridge_mid_mse = np.mean(ridge_mid_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(ridge_mid_mae, ridge_mid_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ec18b",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349910d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct 10-fold cross-validation on the parameter grid to select hyperparameters for the random forest\n",
    "# rf_mid_param_grid = {'max_depth': [1, 2, 3, 4, 5, 6, 10],\n",
    "#                     'n_estimators': [50, 100, 200, 500, 1000],\n",
    "#                     'max_features': [None, 'log2', 'sqrt']}\n",
    "rf_mid_param_grid = {'max_depth': [1, 2],\n",
    "                    'n_estimators': [50, 100],\n",
    "                    'max_features': [None, 'log2', 'sqrt']}\n",
    "scoring_metrics = ['neg_mean_absolute_error', 'neg_mean_squared_error']\n",
    "rf_mid_cv_results = GridSearchCV(RandomForestRegressor(), \n",
    "                                rf_mid_param_grid, \n",
    "                                scoring=scoring_metrics,\n",
    "                                cv=10,\n",
    "                                refit=False)\n",
    "rf_mid_cv_results.fit(X_train_mid1, y_train_mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508991c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = rf_mid_cv_results.cv_results_\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "fig.subplots_adjust(hspace=0.3, top=0.92)\n",
    "\n",
    "for i, max_features in enumerate(rf_mid_param_grid['max_features']):\n",
    "    for n in rf_mid_param_grid['n_estimators']:\n",
    "        data_idx_bool = ((cv_results['param_max_features'] == max_features) & (cv_results['param_n_estimators'] == n))\n",
    "        depth_data = cv_results['param_max_depth'][data_idx_bool]\n",
    "        \n",
    "        mae_data = -1*cv_results['mean_test_neg_mean_absolute_error'][data_idx_bool]\n",
    "        # mae_error_data = cv_results['std_test_neg_mean_absolute_error'][data_idx_bool]\n",
    "        \n",
    "        mse_data = -1*cv_results['mean_test_neg_mean_squared_error'][data_idx_bool]\n",
    "        # mse_error_data = cv_results['std_test_neg_mean_squared_error'][data_idx_bool]\n",
    "        \n",
    "        axs[i, 0].plot(depth_data, mae_data)\n",
    "        axs[i, 1].plot(depth_data, mse_data)\n",
    "        \n",
    "        axs[i, 0].set_ylabel(\"Mean absolute error\")\n",
    "        axs[i, 1].set_ylabel(\"Mean squared error\")\n",
    "    \n",
    "axs[0, 0].set_xlabel(\"Max depth\")\n",
    "axs[0, 1].set_xlabel(\"Max depth\")\n",
    "\n",
    "# Add title and subtitles to figure\n",
    "fig.text(0.5, 0.94, \n",
    "         f\"Max features: '{rf_mid_param_grid['max_features'][0]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "fig.text(0.5, 0.65, \n",
    "         f\"Max features: '{rf_mid_param_grid['max_features'][1]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "fig.text(0.5, 0.355, \n",
    "         f\"Max features: '{rf_mid_param_grid['max_features'][2]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "# for n in n_estimators_list:\n",
    "#     axs[0].plot(max_depth, rf_mid_cv_mae[n]['train'], label=\"{}\".format(n))\n",
    "#     axs[1].plot(max_depth, rf_mid_cv_mse[n]['train'], label=\"{}\".format(n))\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.set_xscale('log')\n",
    "#     ax.set_xlabel('Maximum tree depth')\n",
    "#     ax.xaxis.set_minor_formatter(FormatStrFormatter(\"%.0f\"))\n",
    "#     ax.tick_params(which='major', labelbottom=False)\n",
    "\n",
    "# axs[0].set_ylabel('MAE')\n",
    "# axs[1].set_ylabel('MSE')\n",
    "# axs[1].legend()\n",
    "\n",
    "# fig.suptitle(\"Random forest training set performance\", fontsize=14)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# axes[2].plot(max_depth, rf_mid_cv_mae[n]['val'], label=\"{}\".format(n))\n",
    "# axes[3].plot(max_depth, rf_mid_cv_mse[n]['val'], label=\"{}\".format(n))\n",
    "\n",
    "# axes[2].set_ylabel('MAE on validation sets')\n",
    "# axes[3].set_ylabel('MSE on validation sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ed5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the random forest regression model with hyperparameters as determined above\n",
    "# max_depth_final = 4\n",
    "# n_estimators_final = 1000\n",
    "# rf_mid_model1 = RandomForestRegressor(max_depth=max_depth_final, random_state=0, oob_score=True, n_estimators=n_estimators_final)\n",
    "# rf_mid_model1.fit(X_train_mid1, y_train_mid)\n",
    "\n",
    "# rf1_mid_predict_train = rf_mid_model1.predict(X_train_mid1)\n",
    "# rf1_mid_predict_test = rf_mid_model1.predict(X_test_mid1)\n",
    "\n",
    "# rf1_mid_resid_train = rf1_mid_predict_train - y_train_mid\n",
    "# rf1_mid_resid_test = rf1_mid_predict_test - y_test_mid\n",
    "\n",
    "# rf1_mid_train_mae = np.mean(abs(resid_train))\n",
    "# rf1_mid_test_mae = np.mean(abs(resid_test))\n",
    "# rf1_mid_train_mse = np.mean(resid_train**2)\n",
    "# rf1_mid_test_mse = np.mean(resid_test**2)\n",
    "\n",
    "# print(\"MAE on test set = {}\\nMSE on test set = {}\".format(rf1_mid_test_mae, rf1_mid_test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3242796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine feature importance\n",
    "# feature_importances = (pd.Series(rf_mid_model1.feature_importances_, index=X_train_mid1.columns)\n",
    "#                        .sort_values()\n",
    "#                        .to_frame(name='importance'))\n",
    "# feature_importances['rank'] = feature_importances.importance.rank(ascending=False)\n",
    "# fig, ax = plt.subplots(figsize=(8, 52))\n",
    "# ax.margins(y = 0.0025)\n",
    "# ax.barh(feature_importances.index, feature_importances.importance)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_threshold = 0.005\n",
    "# rf_vars_initial = feature_importances.loc[feature_importances.importance >= importance_threshold].index.tolist()\n",
    "# n_vars = len(rf_vars_initial)\n",
    "# print(\"No. of variables = {}\".format(len(feature_importances)))\n",
    "# print(\"No. of variables above importance threshold = {}\".format(len(rf_vars_initial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b55cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We will only perform the variable selection CV using variables that are above the importance threshold in the initial model\n",
    "# # since running this for all variables would be time-consuming although theoretically preferred.\n",
    "\n",
    "# # Set up the variable selection cross-validation\n",
    "# validation_sets = split(X_train_mid1.index, 10)\n",
    "# max_depth = 4\n",
    "# n_estimators_list = [1000]\n",
    "# cv_info = {n: {} for n in n_estimators_list}\n",
    "\n",
    "# for n_estimators in n_estimators_list:\n",
    "#     Z_train = X_train_mid1[rf_vars_initial]\n",
    "#     Z_test = X_test_mid1[rf_vars_initial]\n",
    "#     rf_vars_current = rf_vars_initial.copy()\n",
    "#     cv_info[n_estimators] = {x: {} for x in range(1, len(rf_vars_initial)+1)}\n",
    "\n",
    "#     while len(rf_vars_current) > 0: \n",
    "#         print(len(rf_vars_current))\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['vars'] = rf_vars_current.copy()\n",
    "\n",
    "#         Z_train = Z_train[rf_vars_current]\n",
    "#         rf_regr_vs = RandomForestRegressor(max_depth=max_depth,\n",
    "#                                             random_state=0,\n",
    "#                                             oob_score=True,\n",
    "#                                             n_estimators=n_estimators)\n",
    "#         rf_regr_vs.fit(Z_train, y_train_mid)\n",
    "\n",
    "#         # Get MAE and MSE estimates using cross-validation\n",
    "#         mae = []\n",
    "#         mse = []\n",
    "#         for validation_idxs in validation_sets:\n",
    "#             Z_train1 = Z_train.drop(validation_idxs, axis=0)\n",
    "#             y_train_mid1 = y_train_mid.drop(validation_idxs)\n",
    "#             Z_val = Z_train.loc[validation_idxs]\n",
    "#             y_val_mid = y_train_mid.loc[validation_idxs]\n",
    "\n",
    "#             rf_regr = RandomForestRegressor(max_depth=max_depth,\n",
    "#                                             random_state=0,\n",
    "#                                             oob_score=True,\n",
    "#                                             n_estimators=n_estimators)\n",
    "#             rf_regr.fit(Z_train1, y_train_mid1)\n",
    "\n",
    "#             predict_val = rf_regr.predict(Z_val)\n",
    "#             resid_val = predict_val - y_val_mid\n",
    "#             mae.append(np.mean(abs(resid_val)))\n",
    "#             mse.append(np.mean(resid_val**2))\n",
    "\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['mae'] = np.mean(mae)\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['mse'] = np.mean(mse)\n",
    "\n",
    "#         var_importances = pd.Series(rf_regr_vs.feature_importances_, index=rf_vars_current).sort_values()\n",
    "#         drop_var = var_importances.index[0]\n",
    "#         rf_vars_current.remove(var_importances.index[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39264a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.linspace(1, n_vars, n_vars)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# for n_estimators in n_estimators_list:\n",
    "#     mae_varselect_cv = [cv_info[n_estimators][x]['mae'] for x in cv_info[n_estimators]]\n",
    "#     mse_varselect_cv = [cv_info[n_estimators][x]['mse'] for x in cv_info[n_estimators]]\n",
    "    \n",
    "#     ax1.plot(idx, mae_varselect_cv, label=\"{}\".format(n_estimators))\n",
    "#     ax2.plot(idx, mse_varselect_cv, label=\"{}\".format(n_estimators))\n",
    "    \n",
    "# ax1.set_xlabel(\"No. of variables\")\n",
    "# ax1.set_ylabel(\"Validation MAE\")\n",
    "# ax1.legend()\n",
    "# ax2.set_xlabel(\"No. of variables\")\n",
    "# ax2.set_ylabel(\"Validation MSE\")\n",
    "# ax2.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the model after selecting variables\n",
    "# n_estimators_final = 1000\n",
    "# n_vars_final = 22\n",
    "# mid_rf_vars_final = cv_info[n_estimators_final][n_vars_final]['vars']\n",
    "# X_train_mid_rf = X_train_mid1[mid_rf_vars_final]\n",
    "# X_test_mid_rf = X_test_mid1[mid_rf_vars_final]\n",
    "\n",
    "# rf_mid_model2 = RandomForestRegressor(max_depth=4, oob_score=True, n_estimators=n_estimators_final)\n",
    "# rf_mid_model2.fit(X_train_mid_rf, y_train_mid)\n",
    "\n",
    "# rf2_mid_test_predict = rf_mid_model2.predict(X_test_mid_rf)\n",
    "# rf2_mid_test_resid = rf2_mid_test_predict - y_test_mid\n",
    "# rf2_mid_test_mae = np.mean(abs(rf2_mid_test_resid))\n",
    "# rf2_mid_test_mse = np.mean(rf2_mid_test_resid**2)\n",
    "# print(\"MAE on test set = {:.3f}\\nMSE on test set = {:.3f}\".format(rf2_mid_test_mae, rf2_mid_test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a1dc90",
   "metadata": {},
   "source": [
    "# Forward modelling\n",
    "\n",
    "The process for modelling the points scored by forwards will be the same as for the goalkeepers. In order to avoid labouring points, I will mostly go through the process with little explanation, commenting only where instructive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e906185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated columns\n",
    "drop_cols = ([\"opp_npxGA_{}\".format(x) for x in range(1, 5)] +\n",
    "             [\"opp_npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"team_npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"team_npxGA_{}\".format(x) for x in range(1, 5)] +\n",
    "             [\"selected_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"value_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"npxG_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"npg_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"starts_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"influence_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"creativity_{}\".format(x) for x in range(1, 5)] + \n",
    "             [\"bps_{}\".format(x) for x in range(1, 5)]\n",
    "             )\n",
    "\n",
    "X_train_fwd2 = X_train_fwd1.drop(drop_cols, axis=1)\n",
    "X_test_fwd2 = X_test_fwd1.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb28811",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44e356",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit an initial linear model to the training data\n",
    "ols_model_fwd = sm.OLS(y_train_fwd, X_train_fwd2)\n",
    "ols_res_fwd = ols_model_fwd.fit()\n",
    "\n",
    "ols_fwd_resid = ols_res_fwd.resid_pearson\n",
    "ols_fwd_fv = ols_res_fwd.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ab14c",
   "metadata": {},
   "source": [
    "The QQ plot of standardised residuals vs the theoretical quantiles appears to show significant skew in the distribution of residuals. Much like the plot for the goalkeepers, the plot of standardised residuals appears to indicate heteroskedasticity in the standardised residuals, with smaller variance in residuals for lower fitted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd4078",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gof_plots(y_train_fwd, ols_fwd_resid, ols_fwd_fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c55817",
   "metadata": {},
   "source": [
    "I now want to apply the model to the test data in order to assess its performance. As a baseline model I will use the average value from the training data and assign this as the predicted value for all test set data. Then, we can compare the performance of future models against this uninformed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59652af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test MAE and MSE under the use of the training mean as a constant model\n",
    "fwd_baseline_mae = np.mean(abs(y_train_fwd.mean() - y_test_fwd))\n",
    "fwd_baseline_mse = np.mean((y_train_fwd.mean() - y_test_fwd)**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(fwd_baseline_mae, fwd_baseline_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted values for the test data\n",
    "X_test_fwd2 = X_test_fwd1[X_train_fwd2.columns]\n",
    "ols_fwd_test_predict = ols_res_fwd.predict(X_test_fwd2)\n",
    "ols_fwd_test_resid = ols_fwd_test_predict - y_test_fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83953d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_fwd, ols_fwd_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ols_fwd_test_predict, ols_fwd_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3637e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test MAE and MSE\n",
    "ols_fwd_test_mae = np.mean(abs(ols_fwd_test_resid))\n",
    "ols_fwd_test_mse = np.mean(ols_fwd_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(ols_fwd_test_mae, ols_fwd_test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f610b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 1\n",
    "y_train_fwd_boxcox = y_train_fwd.drop(y_train_fwd.loc[y_train_fwd < 0].index) + shift\n",
    "y_train_fwd_boxcox_index = y_train_fwd_boxcox.index\n",
    "X_train_fwd_boxcox = X_train_fwd2.drop(y_train_fwd.loc[y_train_fwd < 0].index)\n",
    "\n",
    "y_train_fwd_boxcox, lambda_fwd = stats.boxcox(y_train_fwd_boxcox)\n",
    "\n",
    "y_train_fwd_boxcox = pd.Series(y_train_fwd_boxcox, y_train_fwd_boxcox_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stepwise backward elimination, removing variables whose coefficient estimates do not have p-values falling below\n",
    "# the specified threshold\n",
    "priority_list_fwd = priority_list.copy()\n",
    "dropped_cols = []\n",
    "X_train_fwd_temp = X_train_fwd_boxcox.copy()\n",
    "selection_complete = False\n",
    "p_value_threshold = 0.1\n",
    "\n",
    "while not selection_complete:\n",
    "    model_temp = sm.OLS(y_train_fwd_boxcox, X_train_fwd_temp)\n",
    "    res_temp = model_temp.fit()\n",
    "    drop_candidates = res_temp.pvalues.loc[res_temp.pvalues > p_value_threshold]\n",
    "    if len(drop_candidates) == 0:\n",
    "        selection_complete = True\n",
    "        continue\n",
    "    \n",
    "    drop_col = priority_list_fwd.pop([col in drop_candidates for col in priority_list_fwd].index(True))\n",
    "    dropped_cols.append(drop_col)\n",
    "    X_train_fwd_temp.drop(drop_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine model chosen by backward elimination\n",
    "X_train_fwd_bs = sm.add_constant(X_train_fwd_temp)\n",
    "ols_bs_model_fwd = sm.OLS(y_train_fwd_boxcox, X_train_fwd_bs)\n",
    "ols_bs_res_fwd = ols_bs_model_fwd.fit()\n",
    "\n",
    "print(ols_bs_res_fwd.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b341ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_bs_fwd_resid = ols_bs_res_fwd.resid_pearson\n",
    "ols_bs_fwd_fv = ols_bs_res_fwd.fittedvalues\n",
    "\n",
    "gof_plots(y_train_fwd_boxcox, ols_bs_fwd_resid, ols_bs_fwd_fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3c073",
   "metadata": {},
   "source": [
    "The above diagnostic plots show that there has been an improvement in the specification of the model as a result of the transformation of the response variable and the application of the backward elimination variable selection process. The QQ plot indicates that the standardised residuals more closely fit the standard normal distribution; the plot of standardised residuals vs fitted values shows a decrease in clear heteroskedasticity in the residuals; and, although the fitted values vs response plot shows an increase in variablility, this is likely due to overfitting in the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the backward selection model to the test data\n",
    "X_test_fwd_bs = X_test_fwd1[X_train_fwd_bs.columns]\n",
    "\n",
    "# Apply inverse Box-Cox transformation to the predicted values\n",
    "ols_bs_fwd_test_predict_boxcox = ols_bs_res_fwd.predict(X_test_fwd_bs)\n",
    "ols_bs_fwd_test_predict = inverse_boxcox(ols_bs_fwd_test_predict_boxcox, lmbda=lambda_fwd) - 1\n",
    "\n",
    "ols_bs_fwd_test_resid = ols_bs_fwd_test_predict - y_test_fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e451099",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_fwd, ols_bs_fwd_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ols_bs_fwd_test_predict, ols_bs_fwd_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af30a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "mae_bs = np.mean(abs(ols_bs_fwd_test_resid))\n",
    "mse_bs = np.mean(ols_bs_fwd_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(mae_bs, mse_bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757044a",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "We now move on to fitting penalised regressions in order to select important variables. We begin by performing a cross-validation to determine the optimal order of magnitude for the penalisation parameter. For both the lasso and ridge regressions we continue to fit the linear model to the response after having applied the Box-Cox transform as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fad465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a lasso regression implementing 10-fold cross-validation for the selection of the tuning parameter\n",
    "validation_idxs = split(X_train_fwd_boxcox.sample(frac=1).index, 10)\n",
    "lasso_fwd_alpha_list = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "lasso_fwd_cv_error_dict = {'mae': [], 'mse': []}\n",
    "for a in lasso_fwd_alpha_list:\n",
    "    mae = []\n",
    "    mse = []\n",
    "    \n",
    "    for i, idx in enumerate(validation_idxs):\n",
    "        X_val_fwd_cv = X_train_fwd_boxcox.loc[idx]\n",
    "        y_val_fwd_cv = y_train_fwd[idx]\n",
    "        X_train_fwd_cv = X_train_fwd_boxcox.drop(idx, axis=0)\n",
    "        y_train_fwd_cv = y_train_fwd_boxcox.drop(idx)\n",
    "        \n",
    "        lasso_fwd_model = sm.OLS(y_train_fwd_cv, X_train_fwd_cv)\n",
    "        lasso_fwd_res = lasso_fwd_model.fit_regularized(alpha=a, L1_wt=1)\n",
    "        \n",
    "        lasso_fwd_cv_predict_boxcox = lasso_fwd_res.predict(X_val_fwd_cv)\n",
    "        lasso_fwd_cv_predict = inverse_boxcox(lasso_fwd_cv_predict_boxcox, lmbda=lambda_fwd) - 1\n",
    "        \n",
    "        mae.append(np.mean(abs(lasso_fwd_cv_predict - y_val_fwd_cv)))\n",
    "        mse.append(np.mean((lasso_fwd_cv_predict - y_val_fwd_cv)**2))\n",
    "        \n",
    "    lasso_fwd_cv_error_dict['mae'].append(np.mean(mae))\n",
    "    lasso_fwd_cv_error_dict['mse'].append(np.mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MAE and MSE for different values of the tuning parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.plot(lasso_fwd_alpha_list, lasso_fwd_cv_error_dict['mae'])\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Alpha\")\n",
    "ax1.set_ylabel(\"Mean absolute error\")\n",
    "\n",
    "ax2.plot(lasso_fwd_alpha_list, lasso_fwd_cv_error_dict['mse'])\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Alpha\")\n",
    "ax2.set_ylabel(\"Mean square error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9548c03",
   "metadata": {},
   "source": [
    "The plots above indicate that the best two options for the penalisation parameter, `alpha`, would be 0.1, which minimizes the mean absolute error, or 0.01, which minimizes the mean square error. Note that, here, the larger the size of the parameter the stricter the penalisation of the regression coefficients. I have chosen to use `alpha` = 0.01 as the minimizer of the mean square error on the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c100c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final lasso model\n",
    "lasso_fwd_alpha = 1e-2\n",
    "\n",
    "lasso_fwd_model = sm.OLS(y_train_fwd_boxcox, X_train_fwd_boxcox)\n",
    "lasso_fwd_res = lasso_fwd_model.fit_regularized(alpha=lasso_fwd_alpha, L1_wt=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0070d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the response on the test set\n",
    "lasso_fwd_test_predict_boxcox = lasso_fwd_res.predict(X_test_fwd2)\n",
    "lasso_fwd_test_predict = inverse_boxcox(lasso_fwd_test_predict_boxcox, lmbda=lambda_fwd)\n",
    "lasso_fwd_test_resid = lasso_fwd_test_predict - y_test_fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_fwd, lasso_fwd_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(lasso_fwd_test_predict, lasso_fwd_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8412d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "lasso_fwd_mae = np.mean(abs(lasso_fwd_test_resid))\n",
    "lasso_fwd_mse = np.mean(lasso_fwd_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(lasso_fwd_mae, lasso_fwd_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e5344",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ee7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a ridge regression implementing 10-fold cross-validation for the selection of the tuning parameter\n",
    "ridge_fwd_alpha_list = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "ridge_fwd_cv_error_dict = {'mae': [], 'mse': []}\n",
    "for a in ridge_fwd_alpha_list:\n",
    "    mae = []\n",
    "    mse = []\n",
    "    \n",
    "    for i, idx in enumerate(validation_idxs):\n",
    "        X_val_fwd_cv = X_train_fwd_boxcox.loc[idx]\n",
    "        y_val_fwd_cv = y_train_fwd[idx]\n",
    "        X_train_fwd_cv = X_train_fwd_boxcox.drop(idx, axis=0)\n",
    "        y_train_fwd_cv = y_train_fwd_boxcox.drop(idx)\n",
    "        \n",
    "        ridge_fwd_model = sm.OLS(y_train_fwd_cv, X_train_fwd_cv)\n",
    "        ridge_fwd_res = ridge_fwd_model.fit_regularized(alpha=a, L1_wt=0)\n",
    "        \n",
    "        ridge_fwd_cv_predict_boxcox = ridge_fwd_res.predict(X_val_fwd_cv)\n",
    "        ridge_fwd_cv_predict = inverse_boxcox(ridge_fwd_cv_predict_boxcox, lmbda=lambda_fwd) - 1\n",
    "        \n",
    "        mae.append(np.mean(abs(ridge_fwd_cv_predict - y_val_fwd_cv)))\n",
    "        mse.append(np.mean((ridge_fwd_cv_predict - y_val_fwd_cv)**2))\n",
    "        \n",
    "    ridge_fwd_cv_error_dict['mae'].append(np.mean(mae))\n",
    "    ridge_fwd_cv_error_dict['mse'].append(np.mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MAE and MSE on the validation sets for different values of the tuning parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.plot(ridge_fwd_alpha_list, ridge_fwd_cv_error_dict['mae'])\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Alpha\")\n",
    "ax1.set_ylabel(\"Mean absolute error\")\n",
    "\n",
    "ax2.plot(ridge_fwd_alpha_list, ridge_fwd_cv_error_dict['mse'])\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Alpha\")\n",
    "ax2.set_ylabel(\"Mean square error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed79b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final ridge model\n",
    "ridge_fwd_alpha = 1e-2\n",
    "\n",
    "ridge_fwd_model = sm.OLS(y_train_fwd_boxcox, X_train_fwd_boxcox)\n",
    "ridge_fwd_res = ridge_fwd_model.fit_regularized(alpha=ridge_fwd_alpha, L1_wt=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05821a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the response on the test set\n",
    "ridge_fwd_test_predict_boxcox = ridge_fwd_res.predict(X_test_fwd2)\n",
    "ridge_fwd_test_predict = inverse_boxcox(ridge_fwd_test_predict_boxcox, lmbda=lambda_fwd)\n",
    "ridge_fwd_test_resid = ridge_fwd_test_predict - y_test_fwd\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot of predicted values vs actual values\n",
    "ax1.scatter(y_test_fwd, ridge_fwd_test_predict, edgecolor='k', linewidths=0.2)\n",
    "ax1.set_xlabel(\"Response\")\n",
    "ax1.set_ylabel(\"Predicted values\")\n",
    "ax1.set_title(\"Predicted values vs response\")\n",
    "\n",
    "# Plot of residuals vs fitted values\n",
    "ax2.scatter(ridge_fwd_test_predict, ridge_fwd_test_resid, edgecolor='k', linewidths=0.2)\n",
    "ax2.set_xlabel(\"Predicted values\")\n",
    "ax2.set_ylabel(\"Raw residuals\")\n",
    "ax2.set_title(\"Raw residuals vs predicted values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOS MAE and MSE\n",
    "ridge_fwd_mae = np.mean(abs(ridge_fwd_test_resid))\n",
    "ridge_fwd_mse = np.mean(ridge_fwd_test_resid**2)\n",
    "print(\"MAE: {:.3f}\\nMSE: {:.3f}\".format(ridge_fwd_mae, ridge_fwd_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969fc9f",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880cf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct 10-fold cross-validation on the parameter grid to select hyperparameters for the random forest\n",
    "# rf_fwd_param_grid = {'max_depth': [1, 2, 3, 4, 5, 6, 10],\n",
    "#                     'n_estimators': [50, 100, 200, 500, 1000],\n",
    "#                     'max_features': [None, 'log2', 'sqrt']}\n",
    "rf_fwd_param_grid = {'max_depth': [1, 2],\n",
    "                    'n_estimators': [50, 100],\n",
    "                    'max_features': [None, 'log2', 'sqrt']}\n",
    "scoring_metrics = ['neg_mean_absolute_error', 'neg_mean_squared_error']\n",
    "rf_fwd_cv_results = GridSearchCV(RandomForestRegressor(), \n",
    "                                rf_fwd_param_grid, \n",
    "                                scoring=scoring_metrics,\n",
    "                                cv=10,\n",
    "                                refit=False)\n",
    "rf_fwd_cv_results.fit(X_train_fwd1, y_train_fwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101ef2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = rf_fwd_cv_results.cv_results_\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "fig.subplots_adjust(hspace=0.3, top=0.92)\n",
    "\n",
    "for i, max_features in enumerate(rf_fwd_param_grid['max_features']):\n",
    "    for n in rf_fwd_param_grid['n_estimators']:\n",
    "        data_idx_bool = ((cv_results['param_max_features'] == max_features) & (cv_results['param_n_estimators'] == n))\n",
    "        depth_data = cv_results['param_max_depth'][data_idx_bool]\n",
    "        \n",
    "        mae_data = -1*cv_results['mean_test_neg_mean_absolute_error'][data_idx_bool]\n",
    "        # mae_error_data = cv_results['std_test_neg_mean_absolute_error'][data_idx_bool]\n",
    "        \n",
    "        mse_data = -1*cv_results['mean_test_neg_mean_squared_error'][data_idx_bool]\n",
    "        # mse_error_data = cv_results['std_test_neg_mean_squared_error'][data_idx_bool]\n",
    "        \n",
    "        axs[i, 0].plot(depth_data, mae_data)\n",
    "        axs[i, 1].plot(depth_data, mse_data)\n",
    "        \n",
    "        axs[i, 0].set_ylabel(\"Mean absolute error\")\n",
    "        axs[i, 1].set_ylabel(\"Mean squared error\")\n",
    "    \n",
    "axs[0, 0].set_xlabel(\"Max depth\")\n",
    "axs[0, 1].set_xlabel(\"Max depth\")\n",
    "\n",
    "# Add title and subtitles to figure\n",
    "fig.text(0.5, 0.94, \n",
    "         f\"Max features: '{rf_fwd_param_grid['max_features'][0]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "fig.text(0.5, 0.65, \n",
    "         f\"Max features: '{rf_fwd_param_grid['max_features'][1]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "fig.text(0.5, 0.355, \n",
    "         f\"Max features: '{rf_fwd_param_grid['max_features'][2]}'\", \n",
    "         fontsize=16, \n",
    "         horizontalalignment='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "# for n in n_estimators_list:\n",
    "#     axs[0].plot(max_depth, rf_fwd_cv_mae[n]['train'], label=\"{}\".format(n))\n",
    "#     axs[1].plot(max_depth, rf_fwd_cv_mse[n]['train'], label=\"{}\".format(n))\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.set_xscale('log')\n",
    "#     ax.set_xlabel('Maximum tree depth')\n",
    "#     ax.xaxis.set_minor_formatter(FormatStrFormatter(\"%.0f\"))\n",
    "#     ax.tick_params(which='major', labelbottom=False)\n",
    "\n",
    "# axs[0].set_ylabel('MAE')\n",
    "# axs[1].set_ylabel('MSE')\n",
    "# axs[1].legend()\n",
    "\n",
    "# fig.suptitle(\"Random forest training set performance\", fontsize=14)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# axes[2].plot(max_depth, rf_fwd_cv_mae[n]['val'], label=\"{}\".format(n))\n",
    "# axes[3].plot(max_depth, rf_fwd_cv_mse[n]['val'], label=\"{}\".format(n))\n",
    "\n",
    "# axes[2].set_ylabel('MAE on validation sets')\n",
    "# axes[3].set_ylabel('MSE on validation sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the random forest regression model with hyperparameters as determined above\n",
    "# max_depth_final = 4\n",
    "# n_estimators_final = 1000\n",
    "# rf_fwd_model1 = RandomForestRegressor(max_depth=max_depth_final, random_state=0, oob_score=True, n_estimators=n_estimators_final)\n",
    "# rf_fwd_model1.fit(X_train_fwd1, y_train_fwd)\n",
    "\n",
    "# rf1_fwd_predict_train = rf_fwd_model1.predict(X_train_fwd1)\n",
    "# rf1_fwd_predict_test = rf_fwd_model1.predict(X_test_fwd1)\n",
    "\n",
    "# rf1_fwd_resid_train = rf1_fwd_predict_train - y_train_fwd\n",
    "# rf1_fwd_resid_test = rf1_fwd_predict_test - y_test_fwd\n",
    "\n",
    "# rf1_fwd_train_mae = np.mean(abs(resid_train))\n",
    "# rf1_fwd_test_mae = np.mean(abs(resid_test))\n",
    "# rf1_fwd_train_mse = np.mean(resid_train**2)\n",
    "# rf1_fwd_test_mse = np.mean(resid_test**2)\n",
    "\n",
    "# print(\"MAE on test set = {}\\nMSE on test set = {}\".format(rf1_fwd_test_mae, rf1_fwd_test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine feature importance\n",
    "# feature_importances = (pd.Series(rf_fwd_model1.feature_importances_, index=X_train_fwd1.columns)\n",
    "#                        .sort_values()\n",
    "#                        .to_frame(name='importance'))\n",
    "# feature_importances['rank'] = feature_importances.importance.rank(ascending=False)\n",
    "# fig, ax = plt.subplots(figsize=(8, 52))\n",
    "# ax.margins(y = 0.0025)\n",
    "# ax.barh(feature_importances.index, feature_importances.importance)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca21608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_threshold = 0.005\n",
    "# rf_vars_initial = feature_importances.loc[feature_importances.importance >= importance_threshold].index.tolist()\n",
    "# n_vars = len(rf_vars_initial)\n",
    "# print(\"No. of variables = {}\".format(len(feature_importances)))\n",
    "# print(\"No. of variables above importance threshold = {}\".format(len(rf_vars_initial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We will only perform the variable selection CV using variables that are above the importance threshold in the initial model\n",
    "# # since running this for all variables would be time-consuming although theoretically preferred.\n",
    "\n",
    "# # Set up the variable selection cross-validation\n",
    "# validation_sets = split(X_train_fwd1.index, 10)\n",
    "# max_depth = 4\n",
    "# n_estimators_list = [1000]\n",
    "# cv_info = {n: {} for n in n_estimators_list}\n",
    "\n",
    "# for n_estimators in n_estimators_list:\n",
    "#     Z_train = X_train_fwd1[rf_vars_initial]\n",
    "#     Z_test = X_test_fwd1[rf_vars_initial]\n",
    "#     rf_vars_current = rf_vars_initial.copy()\n",
    "#     cv_info[n_estimators] = {x: {} for x in range(1, len(rf_vars_initial)+1)}\n",
    "\n",
    "#     while len(rf_vars_current) > 0: \n",
    "#         print(len(rf_vars_current))\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['vars'] = rf_vars_current.copy()\n",
    "\n",
    "#         Z_train = Z_train[rf_vars_current]\n",
    "#         rf_regr_vs = RandomForestRegressor(max_depth=max_depth,\n",
    "#                                             random_state=0,\n",
    "#                                             oob_score=True,\n",
    "#                                             n_estimators=n_estimators)\n",
    "#         rf_regr_vs.fit(Z_train, y_train_fwd)\n",
    "\n",
    "#         # Get MAE and MSE estimates using cross-validation\n",
    "#         mae = []\n",
    "#         mse = []\n",
    "#         for validation_idxs in validation_sets:\n",
    "#             Z_train1 = Z_train.drop(validation_idxs, axis=0)\n",
    "#             y_train_fwd1 = y_train_fwd.drop(validation_idxs)\n",
    "#             Z_val = Z_train.loc[validation_idxs]\n",
    "#             y_val_fwd = y_train_fwd.loc[validation_idxs]\n",
    "\n",
    "#             rf_regr = RandomForestRegressor(max_depth=max_depth,\n",
    "#                                             random_state=0,\n",
    "#                                             oob_score=True,\n",
    "#                                             n_estimators=n_estimators)\n",
    "#             rf_regr.fit(Z_train1, y_train_fwd1)\n",
    "\n",
    "#             predict_val = rf_regr.predict(Z_val)\n",
    "#             resid_val = predict_val - y_val_fwd\n",
    "#             mae.append(np.mean(abs(resid_val)))\n",
    "#             mse.append(np.mean(resid_val**2))\n",
    "\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['mae'] = np.mean(mae)\n",
    "#         cv_info[n_estimators][len(rf_vars_current)]['mse'] = np.mean(mse)\n",
    "\n",
    "#         var_importances = pd.Series(rf_regr_vs.feature_importances_, index=rf_vars_current).sort_values()\n",
    "#         drop_var = var_importances.index[0]\n",
    "#         rf_vars_current.remove(var_importances.index[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fce41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.linspace(1, n_vars, n_vars)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# for n_estimators in n_estimators_list:\n",
    "#     mae_varselect_cv = [cv_info[n_estimators][x]['mae'] for x in cv_info[n_estimators]]\n",
    "#     mse_varselect_cv = [cv_info[n_estimators][x]['mse'] for x in cv_info[n_estimators]]\n",
    "    \n",
    "#     ax1.plot(idx, mae_varselect_cv, label=\"{}\".format(n_estimators))\n",
    "#     ax2.plot(idx, mse_varselect_cv, label=\"{}\".format(n_estimators))\n",
    "    \n",
    "# ax1.set_xlabel(\"No. of variables\")\n",
    "# ax1.set_ylabel(\"Validation MAE\")\n",
    "# ax1.legend()\n",
    "# ax2.set_xlabel(\"No. of variables\")\n",
    "# ax2.set_ylabel(\"Validation MSE\")\n",
    "# ax2.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the model after selecting variables\n",
    "# n_estimators_final = 1000\n",
    "# n_vars_final = 22\n",
    "# fwd_rf_vars_final = cv_info[n_estimators_final][n_vars_final]['vars']\n",
    "# X_train_fwd_rf = X_train_fwd1[fwd_rf_vars_final]\n",
    "# X_test_fwd_rf = X_test_fwd1[fwd_rf_vars_final]\n",
    "\n",
    "# rf_fwd_model2 = RandomForestRegressor(max_depth=4, oob_score=True, n_estimators=n_estimators_final)\n",
    "# rf_fwd_model2.fit(X_train_fwd_rf, y_train_fwd)\n",
    "\n",
    "# rf2_fwd_test_predict = rf_fwd_model2.predict(X_test_fwd_rf)\n",
    "# rf2_fwd_test_resid = rf2_fwd_test_predict - y_test_fwd\n",
    "# rf2_fwd_test_mae = np.mean(abs(rf2_fwd_test_resid))\n",
    "# rf2_fwd_test_mse = np.mean(rf2_fwd_test_resid**2)\n",
    "# print(\"MAE on test set = {:.3f}\\nMSE on test set = {:.3f}\".format(rf2_fwd_test_mae, rf2_fwd_test_mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
